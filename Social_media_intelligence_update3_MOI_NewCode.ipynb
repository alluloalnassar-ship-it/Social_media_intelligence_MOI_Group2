{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch scikit-learn\n"
      ],
      "metadata": {
        "id": "cUzTFveYaF2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYV3a0mDJ8qt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset B — labeled\n",
        "train_df = pd.read_csv(\"/content/Arabic.csv\")\n",
        "train_df = train_df.dropna(subset=[\"tweet\", \"label\"])\n",
        "\n",
        "# Dataset A — unlabeled\n",
        "real_df = pd.read_csv(\"/content/merged_twitterdata.csv\")\n",
        "real_df = real_df.dropna(subset=[\"text\"])\n"
      ],
      "metadata": {
        "id": "jkxWnDAxKl2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2725cdf4"
      },
      "source": [
        "## Requirement Analysis & Data Organization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    \"not\": 0,\n",
        "    \"offensive\": 1\n",
        "}\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].astype(str).str.strip().str.lower()\n",
        "train_df[\"label_id\"] = train_df[\"label\"].map(label_map)\n",
        "\n",
        "print(train_df[[\"label\", \"label_id\"]].head())\n",
        "print(\"NaN in label_id:\", train_df[\"label_id\"].isna().sum())\n",
        "print(\"Label counts:\\n\", train_df[\"label\"].value_counts())\n"
      ],
      "metadata": {
        "id": "pOuNAlEPc1Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train–Validation–Test Split to Reduce Overfitting\n"
      ],
      "metadata": {
        "id": "SORds76XeVfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_df[\"tweet\"].values\n",
        "y = train_df[\"label_id\"].values\n",
        "\n",
        "# Train (70%) + Temp (30%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Validation (15%) + Test (15%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Val size:\", len(X_val))\n",
        "print(\"Test size:\", len(X_test))\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "import numpy as np\n",
        "print(\"Train:\", np.bincount(y_train))\n",
        "print(\"Val:\", np.bincount(y_val))\n",
        "print(\"Test:\", np.bincount(y_test))\n"
      ],
      "metadata": {
        "id": "j1p3bXqFd2Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "959d27ce"
      },
      "source": [
        "# Setup LLM Environment and Load Pre-trained Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch scikit-learn\n"
      ],
      "metadata": {
        "id": "YtgSAGJUg7hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Environment ready: Transformers, Torch, and Scikit-learn\")\n"
      ],
      "metadata": {
        "id": "1JNy8957hIWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5bcf66e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary libraries are installed, the next step is to import `AutoTokenizer` and `AutoModelForSequenceClassification` from the `transformers` library and then load a pre-trained Arabic transformer model and its tokenizer, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pre-trained Arabic BERT Model and Tokenizer\n"
      ],
      "metadata": {
        "id": "FiKDQM4oiBRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Choose a pre-trained Arabic BERT model\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"Tokenizer for {model_name} loaded successfully.\")\n",
        "\n",
        "# Load model for binary sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2\n",
        ")\n",
        "print(f\"Model for {model_name} loaded successfully.\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model moved to device: {device}\")\n"
      ],
      "metadata": {
        "id": "Mczj_yX1hvZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Class Imbalance (Train Only)\n"
      ],
      "metadata": {
        "id": "xeELeJ66jUuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# Compute sample weights for the training set only\n",
        "class_counts = np.bincount(y_train)              # e.g., [not_count, offensive_count]\n",
        "class_weights = 1.0 / class_counts               # higher weight for minority class\n",
        "sample_weights = class_weights[y_train]          # weight per training sample\n",
        "\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=torch.tensor(sample_weights, dtype=torch.double),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "print(\"Train class counts:\", class_counts)\n",
        "print(\"Train class weights:\", class_weights)\n",
        "print(\"Balanced sampler ready \")\n"
      ],
      "metadata": {
        "id": "YqLE60FyjY3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Data and Build Datasets\n"
      ],
      "metadata": {
        "id": "g7Xz35K1jw6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "max_length = 128\n",
        "\n",
        "# Tokenize Train\n",
        "train_encodings = tokenizer(\n",
        "    list(X_train),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Tokenize Validation\n",
        "val_encodings = tokenizer(\n",
        "    list(X_val),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Tokenize Test\n",
        "test_encodings = tokenizer(\n",
        "    list(X_test),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Convert labels to tensors\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
        "\n",
        "# Build datasets\n",
        "train_dataset = TensorDataset(\n",
        "    train_encodings[\"input_ids\"],\n",
        "    train_encodings[\"attention_mask\"],\n",
        "    y_train_t\n",
        ")\n",
        "\n",
        "val_dataset = TensorDataset(\n",
        "    val_encodings[\"input_ids\"],\n",
        "    val_encodings[\"attention_mask\"],\n",
        "    y_val_t\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    test_encodings[\"input_ids\"],\n",
        "    test_encodings[\"attention_mask\"],\n",
        "    y_test_t\n",
        ")\n",
        "\n",
        "print(\"Datasets ready:\",\n",
        "      len(train_dataset),\n",
        "      len(val_dataset),\n",
        "      len(test_dataset))\n"
      ],
      "metadata": {
        "id": "ixVr5EVdjZCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataLoaders\n"
      ],
      "metadata": {
        "id": "7vvRYOcEkMda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Train loader with balanced sampler\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=train_sampler\n",
        ")\n",
        "\n",
        "# Validation loader\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Test loader\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"DataLoaders ready:\",\n",
        "      len(train_loader),\n",
        "      len(val_loader),\n",
        "      len(test_loader))\n"
      ],
      "metadata": {
        "id": "U7Ym6iFVkYcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop with Validation\n"
      ],
      "metadata": {
        "id": "IUCQ-v-TkpHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # ===== Training =====\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # ===== Validation =====\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = correct / total\n",
        "\n",
        "    print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "iABBB7sbktv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Evaluation on Test Set\n"
      ],
      "metadata": {
        "id": "F9bi7owtm3lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Accuracy\n",
        "test_accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Detailed metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    target_names=[\"not\", \"offensive\"]\n",
        "))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "id": "SGeDj3i1m6uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Inference on Unlabeled Data (real_df)\n"
      ],
      "metadata": {
        "id": "MyKtEjPTpd65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize Unlabeled Data\n"
      ],
      "metadata": {
        "id": "RmZ1b0XGpkUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "max_length = 128  # keep consistent with training\n",
        "\n",
        "real_texts = real_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "real_encodings = tokenizer(\n",
        "    real_texts,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Real data tokenized \")\n",
        "print(\"input_ids:\", real_encodings[\"input_ids\"].shape)\n",
        "print(\"attention_mask:\", real_encodings[\"attention_mask\"].shape)\n"
      ],
      "metadata": {
        "id": "z46FR6Dtpv2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Labels for real_df\n"
      ],
      "metadata": {
        "id": "H-6jglZmp5SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "\n",
        "real_dataset = TensorDataset(\n",
        "    real_encodings[\"input_ids\"],\n",
        "    real_encodings[\"attention_mask\"]\n",
        ")\n",
        "\n",
        "real_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "all_real_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in real_loader:\n",
        "        input_ids, attention_mask = [b.to(device) for b in batch]\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_real_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Map numeric predictions back to label names\n",
        "id2label = {0: \"not\", 1: \"offensive\"}\n",
        "real_df[\"pred_id\"] = all_real_preds\n",
        "real_df[\"pred_label\"] = real_df[\"pred_id\"].map(id2label)\n",
        "\n",
        "print(\"Predictions added \")\n",
        "print(real_df[[\"text\", \"pred_label\"]].head())\n",
        "print(\"\\nPred label counts:\")\n",
        "print(real_df[\"pred_label\"].value_counts())\n"
      ],
      "metadata": {
        "id": "UAnu8qm_p-dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human-Labeled Dataset (Manual Annotation)"
      ],
      "metadata": {
        "id": "weHp7yZuqL78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Inspect Human-Labeled Twitter Data"
      ],
      "metadata": {
        "id": "mqIowWngWDoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_df = pd.read_csv(\n",
        "    \"/content/merged_twitterdata with human classification.csv\",\n",
        "    sep=\";\",\n",
        "    encoding=\"utf-8-sig\",\n",
        "    engine=\"python\"\n",
        ")\n",
        "\n",
        "print(human_df.columns)\n",
        "print(human_df.shape)\n",
        "human_df.head()\n"
      ],
      "metadata": {
        "id": "uxl5mKVcT7tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect Human Label Distribution"
      ],
      "metadata": {
        "id": "Bz6ChMqvZi3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_df[\"classification\"] = human_df[\"classification\"].astype(str).str.strip().str.lower()\n",
        "print(human_df[\"classification\"].value_counts())\n"
      ],
      "metadata": {
        "id": "n6UCKG7cZp0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model vs Human Comparison"
      ],
      "metadata": {
        "id": "ilMUBKe8aVMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Model Predictions for Human-Labeled Tweets"
      ],
      "metadata": {
        "id": "uuO49UAmaXXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "max_length = 128\n",
        "\n",
        "texts = human_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "enc = tokenizer(\n",
        "    texts,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "dataset = TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in loader:\n",
        "        input_ids, attention_mask = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        batch_preds = torch.argmax(outputs.logits, dim=1)\n",
        "        preds.extend(batch_preds.cpu().numpy())\n",
        "\n",
        "id2label = {0: \"not\", 1: \"offensive\"}\n",
        "human_df[\"model_pred\"] = [id2label[p] for p in preds]\n",
        "\n",
        "print(human_df[[\"text\", \"classification\", \"model_pred\"]].head(10))\n",
        "print(\"\\nModel prediction counts:\")\n",
        "print(human_df[\"model_pred\"].value_counts())\n"
      ],
      "metadata": {
        "id": "mOeMCQPRacVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation on Full Human-Labeled Dataset (Unbalanced Test)"
      ],
      "metadata": {
        "id": "JKEfBRq5Ql9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import torch\n",
        "\n",
        "# Encode human-labeled texts (unbalanced)\n",
        "human_enc = tokenizer(\n",
        "    human_df[\"text\"].astype(str).tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Convert labels\n",
        "label_map = {\"not\": 0, \"offensive\": 1}\n",
        "y_human = torch.tensor(\n",
        "    human_df[\"classification\"].map(label_map).values\n",
        ")\n",
        "\n",
        "# Create dataset and loader\n",
        "human_dataset = TensorDataset(\n",
        "    human_enc[\"input_ids\"],\n",
        "    human_enc[\"attention_mask\"],\n",
        "    y_human\n",
        ")\n",
        "\n",
        "human_loader = DataLoader(\n",
        "    human_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in human_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_true.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"Unbalanced Human Test Accuracy:\",\n",
        "      accuracy_score(all_true, all_preds))\n",
        "\n",
        "print(\"\\nClassification Report (Human Unbalanced):\")\n",
        "print(classification_report(all_true, all_preds,\n",
        "                            target_names=[\"not\", \"offensive\"]))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(all_true, all_preds))\n"
      ],
      "metadata": {
        "id": "b9yUa5Q9Qsfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model vs Human Annotation Evaluation"
      ],
      "metadata": {
        "id": "5IvNtUhEbWVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix and Classification Report (Human vs Model)"
      ],
      "metadata": {
        "id": "_1s3kZfDbZmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_true = human_df[\"classification\"]\n",
        "y_pred = human_df[\"model_pred\"]\n",
        "\n",
        "print(\"Classification Report (Human vs Model):\")\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    target_names=[\"not\", \"offensive\"]\n",
        "))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "csuD65LpbgBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/arabert_abuse_model\"\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "print(\"Saved to:\", save_dir)\n"
      ],
      "metadata": {
        "id": "jbEi2GkLeH1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r arabert_abuse_model.zip /content/arabert_abuse_model\n"
      ],
      "metadata": {
        "id": "aAIJoVnTeM6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human-Labeled Dataset as Test Set Only\n",
        "\n",
        "### Create a balanced TEST set from human_df"
      ],
      "metadata": {
        "id": "FhFWuLXlrVbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Work on a clean copy\n",
        "test_df = human_df.copy()\n",
        "\n",
        "# Basic cleaning (ensure correct labels)\n",
        "test_df[\"classification\"] = test_df[\"classification\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Balance the test set: take the same number from each class\n",
        "min_n = test_df[\"classification\"].value_counts().min()\n",
        "\n",
        "balanced_test_df = (\n",
        "    test_df.groupby(\"classification\", group_keys=False)\n",
        "           .apply(lambda x: x.sample(n=min_n, random_state=42))\n",
        "           .sample(frac=1, random_state=42)   # shuffle\n",
        "           .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"Original test distribution:\")\n",
        "print(test_df[\"classification\"].value_counts())\n",
        "\n",
        "print(\"\\nBalanced test distribution:\")\n",
        "print(balanced_test_df[\"classification\"].value_counts())\n",
        "\n",
        "print(\"\\nBalanced test size:\", balanced_test_df.shape)\n",
        "balanced_test_df.head()\n"
      ],
      "metadata": {
        "id": "HiG4hBnirWB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation on Balanced Human-Labeled Test Set"
      ],
      "metadata": {
        "id": "53sCLPCjw9Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Encode balanced test texts\n",
        "test_enc = tokenizer(\n",
        "    balanced_test_df[\"text\"].astype(str).tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Convert labels to numeric\n",
        "label_map = {\"not\": 0, \"offensive\": 1}\n",
        "y_test = torch.tensor(\n",
        "    balanced_test_df[\"classification\"].map(label_map).values\n",
        ")\n",
        "\n",
        "# Create test dataset & loader\n",
        "test_dataset = TensorDataset(\n",
        "    test_enc[\"input_ids\"],\n",
        "    test_enc[\"attention_mask\"],\n",
        "    y_test\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_true.extend(labels.cpu().numpy())\n",
        "\n",
        "# Metrics\n",
        "print(\"Balanced Test Accuracy:\", accuracy_score(all_true, all_preds))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_true, all_preds, target_names=[\"not\", \"offensive\"]))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(all_true, all_preds))\n"
      ],
      "metadata": {
        "id": "lCRn8jtpw7Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Save evaluation results (TXT + CSV)\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Convert lists to arrays (assumes all_true & all_preds already exist)\n",
        "y_true = all_true\n",
        "y_pred = all_preds\n",
        "\n",
        "# -------- TXT report --------\n",
        "txt_path = \"balanced_test_results.txt\"\n",
        "\n",
        "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"Balanced Human-Labeled Test Results\\n\")\n",
        "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"Classification Report:\\n\")\n",
        "    f.write(\n",
        "        classification_report(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            target_names=[\"not\", \"offensive\"]\n",
        "        )\n",
        "    )\n",
        "    f.write(\"\\n\\nConfusion Matrix:\\n\")\n",
        "    f.write(str(confusion_matrix(y_true, y_pred)))\n",
        "\n",
        "print(f\"TXT results saved to: {txt_path}\")\n",
        "\n",
        "# -------- CSV predictions --------\n",
        "csv_path = \"balanced_test_predictions.csv\"\n",
        "\n",
        "results_df = balanced_test_df.copy()\n",
        "results_df[\"true_label\"] = results_df[\"classification\"]\n",
        "results_df[\"pred_label\"] = [\"not\" if p == 0 else \"offensive\" for p in y_pred]\n",
        "\n",
        "results_df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"CSV predictions saved to: {csv_path}\")\n",
        "\n",
        "results_df.head()\n"
      ],
      "metadata": {
        "id": "ihwSBCKNrWMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}