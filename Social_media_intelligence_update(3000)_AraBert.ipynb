{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch scikit-learn bitsandbytes accelerate peft --upgrade bitsandbytes"
      ],
      "metadata": {
        "id": "cUzTFveYaF2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYV3a0mDJ8qt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/Arabic_Reports(3000).csv\")\n",
        "train_df = train_df.rename(columns={'البلاغ': 'tweet', 'التصنيف': 'label'})\n",
        "train_df = train_df.dropna(subset=[\"tweet\", \"label\"])\n"
      ],
      "metadata": {
        "id": "ydW_2X68kWMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "\n",
        "   \"Profile Hacking Identity Theft\": 0,\n",
        "   \"EWallet Related Fraud\": 1,\n",
        "   \"Fraud CallVishing\": 2\n",
        "}\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].astype(str).str.strip().str.lower()\n",
        "train_df[\"label_id\"] = train_df[\"label\"].map(label_map)\n",
        "\n",
        "print(train_df[[\"label\", \"label_id\"]].head())\n",
        "print(\"NaN in label_id:\", train_df[\"label_id\"].isna().sum())\n",
        "print(\"Label counts:\\n\", train_df[\"label\"].value_counts())\n"
      ],
      "metadata": {
        "id": "pOuNAlEPc1Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train–Validation–Test Split to Reduce Overfitting\n"
      ],
      "metadata": {
        "id": "SORds76XeVfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Correcting the label_map to match the lowercase 'label' column\n",
        "label_map = {\n",
        "   \"profile hacking identity theft\": 0,\n",
        "   \"ewallet related fraud\": 1,\n",
        "   \"fraud callvishing\": 2\n",
        "}\n",
        "\n",
        "train_df[\"label_id\"] = train_df[\"label\"].map(label_map)\n",
        "\n",
        "# Filter out rows where label_id is NaN, if any non-mappable labels exist\n",
        "train_df_cleaned = train_df.dropna(subset=[\"label_id\"])\n",
        "\n",
        "X = train_df_cleaned[\"tweet\"].values\n",
        "y = train_df_cleaned[\"label_id\"].values.astype(int) # Convert to int after ensuring no NaNs\n",
        "\n",
        "# Train (70%) + Temp (30%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Validation (15%) + Test (15%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Val size:\", len(X_val))\n",
        "print(\"Test size:\", len(X_test))\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "print(\"Train:\", np.bincount(y_train))\n",
        "print(\"Val:\", np.bincount(y_val))\n",
        "print(\"Test:\", np.bincount(y_test))"
      ],
      "metadata": {
        "id": "j1p3bXqFd2Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "959d27ce"
      },
      "source": [
        "# Setup LLM Environment and Load Pre-trained Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch scikit-learn bitsandbytes accelerate peft --upgrade bitsandbytes"
      ],
      "metadata": {
        "id": "YtgSAGJUg7hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Environment ready: Transformers, Torch, and Scikit-learn\")\n"
      ],
      "metadata": {
        "id": "1JNy8957hIWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pre-trained Arabic BERT Model and Tokenizer\n"
      ],
      "metadata": {
        "id": "FiKDQM4oiBRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Choose a pre-trained Arabic BERT model\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"Tokenizer for {model_name} loaded successfully.\")\n",
        "\n",
        "# Load model for binary sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3\n",
        ")\n",
        "print(f\"Model for {model_name} loaded successfully.\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model moved to device: {device}\")\n"
      ],
      "metadata": {
        "id": "Mczj_yX1hvZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Data and Build Datasets\n"
      ],
      "metadata": {
        "id": "g7Xz35K1jw6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "max_length = 128\n",
        "\n",
        "# Tokenize Train\n",
        "train_encodings = tokenizer(\n",
        "    list(X_train),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Tokenize Validation\n",
        "val_encodings = tokenizer(\n",
        "    list(X_val),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Tokenize Test\n",
        "test_encodings = tokenizer(\n",
        "    list(X_test),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Convert labels to tensors\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
        "\n",
        "# Build datasets\n",
        "train_dataset = TensorDataset(\n",
        "    train_encodings[\"input_ids\"],\n",
        "    train_encodings[\"attention_mask\"],\n",
        "    y_train_t\n",
        ")\n",
        "\n",
        "val_dataset = TensorDataset(\n",
        "    val_encodings[\"input_ids\"],\n",
        "    val_encodings[\"attention_mask\"],\n",
        "    y_val_t\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    test_encodings[\"input_ids\"],\n",
        "    test_encodings[\"attention_mask\"],\n",
        "    y_test_t\n",
        ")\n",
        "\n",
        "print(\"Datasets ready:\",\n",
        "      len(train_dataset),\n",
        "      len(val_dataset),\n",
        "      len(test_dataset))\n"
      ],
      "metadata": {
        "id": "ixVr5EVdjZCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataLoaders\n"
      ],
      "metadata": {
        "id": "7vvRYOcEkMda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Train loader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True  # يفضل shuffle للتدريب\n",
        ")\n",
        "\n",
        "# Validation loader\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Test loader\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"DataLoaders ready:\",\n",
        "      len(train_loader),\n",
        "      len(val_loader),\n",
        "      len(test_loader))\n"
      ],
      "metadata": {
        "id": "U7Ym6iFVkYcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop with Validation\n"
      ],
      "metadata": {
        "id": "IUCQ-v-TkpHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # ===== Training =====\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # ===== Validation =====\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = correct / total\n",
        "\n",
        "    print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "iABBB7sbktv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "اضافة _   # ===== Early Stopping ====="
      ],
      "metadata": {
        "id": "pKx8cKiwoxG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# ===== Optimizer =====\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# ===== Training parameters =====\n",
        "epochs = 20\n",
        "patience = 3  # عدد epochs بدون تحسن قبل الإيقاف المبكر\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# مكان حفظ أفضل نموذج\n",
        "best_model_path = \"best_model.pt\"\n",
        "\n",
        "# لتخزين القيم للرسم لاحقًا\n",
        "train_losses_list = []\n",
        "val_losses_list = []\n",
        "val_acc_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # ===== Training =====\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_losses_list.append(avg_train_loss)\n",
        "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # ===== Validation =====\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = correct / total\n",
        "    val_losses_list.append(avg_val_loss)\n",
        "    val_acc_list.append(val_accuracy)\n",
        "\n",
        "    print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # ===== Early Stopping =====\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), best_model_path)  # حفظ أفضل نموذج\n",
        "        print(\"Best model saved!\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"EarlyStopping counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "XFI1ugspou1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Evaluation on Test Set\n"
      ],
      "metadata": {
        "id": "F9bi7owtm3lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Accuracy\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
        "\n",
        "# ترتيب الأسماء حسب label_map\n",
        "label_map = {\n",
        "    \"Profile Hacking Identity Theft\": 0,\n",
        "    \"EWallet Related Fraud\": 1,\n",
        "    \"Fraud CallVishing\": 2\n",
        "}\n",
        "id2label = {v: k for k, v in label_map.items()}\n",
        "target_names = [id2label[i] for i in range(len(id2label))]\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    target_names=target_names,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "id": "TjWnaJm6pbVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AllData(3000) as test"
      ],
      "metadata": {
        "id": "zrxptI6R6aAy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d89ea46"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the human-labeled dataset\n",
        "# Using the path specified in the instructions and previous successful loading attempts\n",
        "human_df = pd.read_csv('/content/Arabic_Reports(3000).csv')\n",
        "\n",
        "# Rename columns from Arabic to English as per the instructions\n",
        "human_df = human_df.rename(columns={'البلاغ': 'tweet', 'التصنيف': 'label'})\n",
        "\n",
        "# Drop rows with missing values in 'tweet' or 'label'\n",
        "human_df = human_df.dropna(subset=[\"tweet\", \"label\"])\n",
        "\n",
        "# Convert 'label' column to string, strip whitespace, and convert to lowercase\n",
        "human_df[\"label\"] = human_df[\"label\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Map labels to numerical IDs using the existing label_map\n",
        "label_map = {\n",
        "   \"profile hacking identity theft\": 0,\n",
        "   \"ewallet related fraud\": 1,\n",
        "   \"fraud callvishing\": 2\n",
        "}\n",
        "human_df[\"label_id\"] = human_df[\"label\"].map(label_map)\n",
        "\n",
        "# Filter out rows where label_id is NaN, if any non-mappable labels exist\n",
        "human_df = human_df.dropna(subset=[\"label_id\"])\n",
        "\n",
        "print(\"Human-labeled dataset loaded and processed:\")\n",
        "print(human_df.head())\n",
        "print(\"\\nNaN in label_id after mapping and dropping:\", human_df[\"label_id\"].isna().sum())\n",
        "print(\"Label counts after initial processing:\\n\", human_df[\"label\"].value_counts())\n",
        "\n",
        "# Balance the dataset\n",
        "min_samples = human_df[\"label_id\"].value_counts().min()\n",
        "human_df_balanced = human_df.groupby(\"label_id\", group_keys=False).apply(lambda x: x.sample(min_samples, random_state=42)).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nBalanced dataset label counts:\\n\", human_df_balanced[\"label_id\"].value_counts())\n",
        "\n",
        "# Extract X_human_test and y_human_test\n",
        "X_human_test = human_df_balanced[\"tweet\"].values\n",
        "y_human_test = human_df_balanced[\"label_id\"].values.astype(int)\n",
        "\n",
        "print(\"\\nShape of X_human_test:\", X_human_test.shape)\n",
        "print(\"Shape of y_human_test:\", y_human_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "018220e7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the best saved model weights\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print(\"Best model weights loaded successfully.\")\n",
        "\n",
        "# 2. Tokenize the X_human_test data\n",
        "human_test_encodings = tokenizer(\n",
        "    list(X_human_test),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "print(\"X_human_test tokenized.\")\n",
        "\n",
        "# 3. Convert y_human_test to a torch.long tensor\n",
        "y_human_test_t = torch.tensor(y_human_test, dtype=torch.long)\n",
        "print(\"y_human_test converted to tensor.\")\n",
        "\n",
        "# 4. Create a TensorDataset named human_test_dataset\n",
        "human_test_dataset = TensorDataset(\n",
        "    human_test_encodings[\"input_ids\"],\n",
        "    human_test_encodings[\"attention_mask\"],\n",
        "    y_human_test_t\n",
        ")\n",
        "print(\"human_test_dataset created.\")\n",
        "\n",
        "# 5. Create a DataLoader named human_test_loader\n",
        "human_test_loader = DataLoader(\n",
        "    human_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "print(\"human_test_loader created.\")\n",
        "\n",
        "# 6. Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# 7. Initialize empty lists to store all predictions and true labels\n",
        "all_human_preds = []\n",
        "all_human_labels = []\n",
        "\n",
        "# 8. Iterate through the human_test_loader in a torch.no_grad() block\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(human_test_loader, desc=\"Evaluating Human Test Set\"):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_human_preds.extend(preds.cpu().numpy())\n",
        "        all_human_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 9. Calculate and print accuracy score and macro F1-score\n",
        "human_test_accuracy = accuracy_score(all_human_labels, all_human_preds)\n",
        "human_macro_f1 = f1_score(all_human_labels, all_human_preds, average='macro')\n",
        "\n",
        "print(f\"\\nHuman Test Set Accuracy: {human_test_accuracy:.4f}\")\n",
        "print(f\"Human Test Set Macro F1-score: {human_macro_f1:.4f}\")\n",
        "\n",
        "# Prepare target names for classification report\n",
        "id2label = {v: k for k, v in label_map.items()}\n",
        "target_names = [id2label[i] for i in range(len(id2label))]\n",
        "\n",
        "# 10. Print the classification report\n",
        "print(\"\\nHuman Test Set Classification Report:\")\n",
        "print(classification_report(\n",
        "    all_human_labels,\n",
        "    all_human_preds,\n",
        "    target_names=target_names,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# 11. Print the confusion matrix\n",
        "print(\"\\nHuman Test Set Confusion Matrix:\")\n",
        "print(confusion_matrix(all_human_labels, all_human_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c25f70"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The model's performance on the balanced human-labeled test set is very strong, achieving a high accuracy and macro F1-score, indicating robust performance across all classes.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The model achieved an accuracy of 0.9760 and a macro F1-score of 0.9760 on the balanced human-labeled test set.\n",
        "*   Per-class performance was strong, with 'ewallet related fraud' showing exceptional metrics (Precision: 0.9980, Recall: 0.9950, F1-score: 0.9965).\n",
        "*   The class 'profile hacking identity theft' had a precision of 0.9576, recall of 0.9720, and F1-score of 0.9648.\n",
        "*   The class 'fraud callvishing' had a precision of 0.9727, recall of 0.9610, and F1-score of 0.9668.\n",
        "*   The confusion matrix revealed that the primary misclassifications occurred between 'fraud callvishing' and 'profile hacking identity theft', with 39 instances of 'fraud callvishing' incorrectly classified as 'profile hacking identity theft', and 26 instances of 'profile hacking identity theft' misclassified as 'fraud callvishing'.\n",
        "*   Evaluation results, including accuracy, classification report, and confusion matrix, were saved to `human_test_evaluation_results.txt`.\n",
        "*   True labels and predicted labels were saved to `human_test_predictions.csv` for further detailed analysis.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Investigate the specific characteristics of the 65 misclassified instances between 'fraud callvishing' and 'profile hacking identity theft' to identify patterns or ambiguous features that confuse the model.\n",
        "*   Consider error analysis on the `human_test_predictions.csv` file to understand the types of errors and potentially refine the model or data preprocessing for these specific cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Qwen"
      ],
      "metadata": {
        "id": "9hyR5_CQ6mOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت أو تحديث bitsandbytes\n",
        "!pip install -U bitsandbytes\n",
        "# تحديث transformers أيضاً\n",
        "!pip install -U transformers\n",
        "# إعادة تشغيل الكيرنل بعد التثبيت\n"
      ],
      "metadata": {
        "id": "F1kP7hD9G4na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n"
      ],
      "metadata": {
        "id": "FlTOTB-D6pQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# إعداد البيئة\n",
        "# =========================\n",
        "!pip install -q transformers accelerate bitsandbytes --upgrade\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "# =========================\n",
        "# تحميل البيانات\n",
        "# =========================\n",
        "df = pd.read_csv(\"/content/Arabic_Reports(3000).csv\")\n",
        "\n",
        "df = df.rename(columns={'البلاغ': 'tweet', 'التصنيف': 'label'})\n",
        "df = df.dropna(subset=[\"tweet\", \"label\"])\n",
        "df[\"label\"] = df[\"label\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "label_map = {\n",
        "    \"profile hacking identity theft\": 0,\n",
        "    \"ewallet related fraud\": 1,\n",
        "    \"fraud callvishing\": 2\n",
        "}\n",
        "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
        "df = df.dropna(subset=[\"label_id\"])\n",
        "\n",
        "X = df[\"tweet\"].values\n",
        "y = df[\"label_id\"].values.astype(int)\n",
        "\n",
        "# تقسيم البيانات 70-15-15\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, stratify=y, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "9qcNVZtdHdZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ تحميل الموديل Qwen\n",
        "# -------------------------------\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")  # ضبط padding_left\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "_IPUatO8PYrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 5️⃣ تجهيز Tokenization\n",
        "# ================================\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "\n",
        "def tokenize_data(texts):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_data(X_train)\n",
        "val_encodings   = tokenize_data(X_val)\n",
        "test_encodings  = tokenize_data(X_test)\n",
        "\n",
        "# تحويل التصنيفات إلى Tensors\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_t   = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "JE6Q0E8rPZIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            preds = outputs.logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = correct/total\n",
        "    print(f\"Epoch {epoch+1} Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), \"best_qwen2_cls.pt\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early Stopping Triggered\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "5-3djouYPZOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_qwen2_cls.pt\"))\n",
        "model.eval()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = outputs.logits.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"Test Accuracy\", accuracy_score(all_labels, all_preds))\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "id": "oeNx5XosQX5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}