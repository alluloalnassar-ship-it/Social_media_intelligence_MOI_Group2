{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzkCDxs_p6s1"
      },
      "source": [
        "# Qwen3-4B Offensive/Not Classification (Arabic Tweets)\n",
        "This notebook is structured to avoid the issues we faced:\n",
        "- NumPy/SciPy binary mismatch\n",
        "- `TrainingArguments` parameter name differences (`evaluation_strategy` vs `eval_strategy`)\n",
        "- Class imbalance handled **ONLY on Train** using `WeightedRandomSampler`\n",
        "- Stratified split implemented **without scikit-learn** (works even if sklearn breaks)\n"
      ],
      "id": "NzkCDxs_p6s1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v_Ocez5p6s4"
      },
      "source": [
        "## 0) Environment Fix (Run once) ÿ´ŸÖ Restart Runtime\n",
        "Run this cell **only once** if you see SciPy/NumPy binary errors. After it finishes, **Restart runtime**.\n"
      ],
      "id": "6v_Ocez5p6s4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCnm9SpEp6s4"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Step 0: Fix binary mismatch (NumPy/SciPy)\n",
        "# Run once, then RESTART runtime.\n",
        "# =========================\n",
        "\n",
        "!pip -q uninstall -y numpy scipy scikit-learn\n",
        "!pip -q install --no-cache-dir -U \"numpy==1.26.4\" \"scipy==1.11.4\" \"scikit-learn==1.4.2\"\n",
        "\n",
        "!pip -q install --no-cache-dir -U transformers accelerate datasets peft trl bitsandbytes evaluate\n",
        "\n",
        "import numpy as np, scipy, sklearn, transformers\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"SciPy:\", scipy.__version__)\n",
        "print(\"sklearn:\", sklearn.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "\n",
        "print(\"‚úÖ If this cell was run to fix errors, now do: Runtime -> Restart runtime\")\n"
      ],
      "id": "iCnm9SpEp6s4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbAgqcFLp6s6"
      },
      "source": [
        "## 1) Imports + Global Seed\n"
      ],
      "id": "vbAgqcFLp6s6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooHu23lkp6s6"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Step 1: Imports + Seed\n",
        "# =========================\n",
        "import inspect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "id": "ooHu23lkp6s6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAoo4dqkp6s7"
      },
      "source": [
        "## 2) Load Data + Clean + Map Labels\n",
        "- Dataset B: labeled (Arabic.csv) -> columns: tweet, label\n",
        "- Dataset A: unlabeled (merged_twitterdata.csv) -> column: text\n"
      ],
      "id": "EAoo4dqkp6s7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY8l45p8p6s8"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Step 2: Load datasets\n",
        "# =========================\n",
        "\n",
        "# Dataset B (labeled)\n",
        "train_df = pd.read_csv(\"/content/Arabic.csv\").dropna(subset=[\"tweet\", \"label\"])\n",
        "\n",
        "# Dataset A (unlabeled)\n",
        "real_df = pd.read_csv(\"/content/merged_twitterdata.csv\").dropna(subset=[\"text\"])\n",
        "\n",
        "# Label mapping\n",
        "label_map = {\"not\": 0, \"offensive\": 1}\n",
        "\n",
        "# Normalize and map labels\n",
        "train_df[\"label\"] = train_df[\"label\"].astype(str).str.strip().str.lower()\n",
        "train_df[\"label_id\"] = train_df[\"label\"].map(label_map)\n",
        "\n",
        "# Drop unknown labels (safety)\n",
        "train_df = train_df.dropna(subset=[\"label_id\"]).copy()\n",
        "train_df[\"label_id\"] = train_df[\"label_id\"].astype(int)\n",
        "\n",
        "print(train_df[[\"label\", \"label_id\"]].head())\n",
        "print(\"NaN in label_id:\", train_df[\"label_id\"].isna().sum())\n",
        "print(\"Label counts:\\n\", train_df[\"label\"].value_counts())\n"
      ],
      "id": "VY8l45p8p6s8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r0IGXm8p6s8"
      },
      "source": [
        "## 3) Stratified Train/Val/Test Split (No scikit-learn)\n",
        "This avoids sklearn import issues.\n"
      ],
      "id": "7r0IGXm8p6s8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7EhF8wkp6s9"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Step 3: Stratified split WITHOUT scikit-learn\n",
        "# 80% train, 10% val, 10% test\n",
        "# =========================\n",
        "\n",
        "def stratified_split(df, label_col=\"label_id\", train_size=0.8, val_size=0.1, test_size=0.1, seed=42):\n",
        "    assert abs(train_size + val_size + test_size - 1.0) < 1e-9, \"Splits must sum to 1.0\"\n",
        "\n",
        "    train_parts, val_parts, test_parts = [], [], []\n",
        "    for _, g in df.groupby(label_col):\n",
        "        g = g.sample(frac=1.0, random_state=seed).reset_index(drop=True)  # shuffle per class\n",
        "        n = len(g)\n",
        "        n_train = int(round(n * train_size))\n",
        "        n_val   = int(round(n * val_size))\n",
        "        n_test  = n - n_train - n_val  # remainder\n",
        "\n",
        "        train_parts.append(g.iloc[:n_train])\n",
        "        val_parts.append(g.iloc[n_train:n_train+n_val])\n",
        "        test_parts.append(g.iloc[n_train+n_val:n_train+n_val+n_test])\n",
        "\n",
        "    train_out = pd.concat(train_parts).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "    val_out   = pd.concat(val_parts).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "    test_out  = pd.concat(test_parts).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    return train_out, val_out, test_out\n",
        "\n",
        "train_part, val_part, test_part = stratified_split(train_df, label_col=\"label_id\", seed=SEED)\n",
        "\n",
        "print(\"Train:\", train_part.shape, \"Val:\", val_part.shape, \"Test:\", test_part.shape)\n",
        "print(\"\\nTrain dist:\\n\", train_part[\"label\"].value_counts(normalize=True))\n",
        "print(\"\\nVal dist:\\n\", val_part[\"label\"].value_counts(normalize=True))\n",
        "print(\"\\nTest dist:\\n\", test_part[\"label\"].value_counts(normalize=True))\n"
      ],
      "id": "l7EhF8wkp6s9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLTgriJ2p6s9"
      },
      "source": [
        "## 4) Prompt Template + HF Datasets\n",
        "We train Qwen (causal LM) to **generate** the label token: `not` or `offensive`.\n"
      ],
      "id": "HLTgriJ2p6s9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI8CnlwAp6s-"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Step 4: Prompt + Datasets\n",
        "# =========================\n",
        "from datasets import Dataset\n",
        "\n",
        "def build_prompt(tweet: str) -> str:\n",
        "    # English comments required\n",
        "    return (\n",
        "        \"Classify the following Arabic tweet into exactly one label: not or offensive.\\n\"\n",
        "        \"Answer with one word only (not/offensive). No explanation.\\n\\n\"\n",
        "        f\"Tweet: {tweet}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "def to_sft_example(row):\n",
        "    prompt = build_prompt(row[\"tweet\"])\n",
        "    completion = \"not\" if int(row[\"label_id\"]) == 0 else \"offensive\"\n",
        "    return {\"prompt\": prompt, \"completion\": completion, \"label_id\": int(row[\"label_id\"])}\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_part.reset_index(drop=True)).map(to_sft_example)\n",
        "val_ds   = Dataset.from_pandas(val_part.reset_index(drop=True)).map(to_sft_example)\n",
        "test_ds  = Dataset.from_pandas(test_part.reset_index(drop=True)).map(to_sft_example)\n",
        "\n",
        "print(\"Train columns:\", train_ds.column_names)\n",
        "print(\"Example:\", train_ds[0])\n"
      ],
      "id": "rI8CnlwAp6s-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIj_XcBpp6s-"
      },
      "source": [
        "## 5) Handle Class Imbalance (Train ONLY)\n",
        "Your counts are roughly `not=7364`, `offensive=3867`.\n",
        "We use `WeightedRandomSampler` so the minority class is sampled more often **during training only**.\n"
      ],
      "id": "DIj_XcBpp6s-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BelpP2SVp6s-"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Step 5: WeightedRandomSampler (train only)\n",
        "# =========================\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "train_labels = np.array(train_ds[\"label_id\"], dtype=int)\n",
        "class_counts = np.bincount(train_labels)\n",
        "class_counts = np.maximum(class_counts, 1)  # safety\n",
        "\n",
        "# Inverse-frequency class weights\n",
        "class_weights = 1.0 / class_counts\n",
        "\n",
        "# Weight per sample\n",
        "sample_weights = class_weights[train_labels]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=torch.tensor(sample_weights, dtype=torch.double),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"Class weights:\", class_weights)\n",
        "print(\"Sampler ready ‚úÖ\")\n"
      ],
      "id": "BelpP2SVp6s-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDzhv7nEp6s_"
      },
      "source": [
        "## 6) Load Qwen/Qwen3-4B (4-bit) + Tokenizer\n"
      ],
      "id": "WDzhv7nEp6s_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6vQ9G0Bp6s_"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Step 6: Load Qwen3-4B in 4-bit\n",
        "# =========================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen3-4B\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        ")\n",
        "\n",
        "# Ensure pad token exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loaded model ‚úÖ\", MODEL_ID)\n"
      ],
      "id": "K6vQ9G0Bp6s_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9LvNzzQp6s_"
      },
      "source": [
        "## 7) LoRA + TrainingArguments (Version-safe)\n",
        "This cell avoids the `evaluation_strategy` error by checking which argument name exists.\n"
      ],
      "id": "p9LvNzzQp6s_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPoSZudCp6tA"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "import inspect\n",
        "import torch\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.10,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        ")\n",
        "\n",
        "def make_training_args(output_dir: str, seed: int):\n",
        "    sig = inspect.signature(TrainingArguments.__init__)\n",
        "    params = sig.parameters\n",
        "\n",
        "    eval_arg_name = \"evaluation_strategy\" if \"evaluation_strategy\" in params else (\n",
        "        \"eval_strategy\" if \"eval_strategy\" in params else None\n",
        "    )\n",
        "\n",
        "    kwargs = dict(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=2e-4,\n",
        "        num_train_epochs=2,\n",
        "        warmup_ratio=0.05,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=0.01,\n",
        "        eval_steps=100,\n",
        "        save_steps=100,\n",
        "        logging_steps=20,\n",
        "        save_total_limit=2,\n",
        "        max_grad_norm=1.0,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        seed=seed,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Enable evaluation\n",
        "    if eval_arg_name is not None:\n",
        "        kwargs[eval_arg_name] = \"steps\"\n",
        "    else:\n",
        "        kwargs[\"do_eval\"] = True\n",
        "\n",
        "    # Mixed precision: pick ONLY ONE\n",
        "    # A100/L4/H100 => bf16 True, fp16 False\n",
        "    if \"bf16\" in params:\n",
        "        kwargs[\"bf16\"] = bool(torch.cuda.is_available())\n",
        "    if \"fp16\" in params:\n",
        "        kwargs[\"fp16\"] = False\n",
        "\n",
        "    # Remove unsupported keys\n",
        "    for k in list(kwargs.keys()):\n",
        "        if k not in params:\n",
        "            kwargs.pop(k)\n",
        "\n",
        "    return TrainingArguments(**kwargs)\n",
        "\n",
        "training_args = make_training_args(\"/content/qwen3_offensive_cls\", seed=SEED)\n",
        "\n",
        "print(\"TrainingArguments ready ‚úÖ\")\n",
        "print(\"bf16:\", getattr(training_args, \"bf16\", None), \"| fp16:\", getattr(training_args, \"fp16\", None))\n",
        "print(\"Eval steps:\", getattr(training_args, \"eval_steps\", None))\n",
        "print(\"Save steps:\", getattr(training_args, \"save_steps\", None))\n"
      ],
      "id": "wPoSZudCp6tA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rLPLF2jp6tA"
      },
      "source": [
        "## 8) Trainer with Weighted Sampler + Train\n",
        "We override `get_train_dataloader()` to apply the sampler.\n"
      ],
      "id": "_rLPLF2jp6tA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUYHD7hUp6tA"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "MAX_LEN = 512  # you can set 256 if you want faster training\n",
        "\n",
        "def tokenize_for_sft(example):\n",
        "    # Build full training text: prompt + completion\n",
        "    text = example[\"prompt\"] + \" \" + example[\"completion\"]\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=False,\n",
        "    )\n",
        "    # Labels must match input_ids for causal LM\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "train_tok = train_ds.map(tokenize_for_sft, remove_columns=train_ds.column_names)\n",
        "val_tok   = val_ds.map(tokenize_for_sft, remove_columns=val_ds.column_names)\n"
      ],
      "id": "iUYHD7hUp6tA"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Causal LM => mlm=False\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
      ],
      "metadata": {
        "id": "75_yI7ILsJw7"
      },
      "id": "75_yI7ILsJw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Collect supported parameters from your installed TRL version\n",
        "trainer_params = set(inspect.signature(SFTTrainer.__init__).parameters.keys())\n",
        "\n",
        "base_kwargs = {\n",
        "    \"model\": model,\n",
        "    \"train_dataset\": train_tok,\n",
        "    \"eval_dataset\": val_tok,\n",
        "    \"args\": training_args,\n",
        "    \"peft_config\": lora_config,\n",
        "    \"data_collator\": data_collator,\n",
        "}\n",
        "\n",
        "# tokenizer argument name differs by version (tokenizer vs processing_class)\n",
        "if \"tokenizer\" in trainer_params:\n",
        "    base_kwargs[\"tokenizer\"] = tokenizer\n",
        "elif \"processing_class\" in trainer_params:\n",
        "    base_kwargs[\"processing_class\"] = tokenizer\n",
        "\n",
        "# Filter out anything not supported (extra safety)\n",
        "trainer_kwargs = {k: v for k, v in base_kwargs.items() if k in trainer_params}\n",
        "\n",
        "print(\"‚úÖ Passing these args to SFTTrainer:\", sorted(trainer_kwargs.keys()))\n"
      ],
      "metadata": {
        "id": "ff8CxVdusNom"
      },
      "id": "ff8CxVdusNom",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"sampler exists:\", \"sampler\" in globals())\n"
      ],
      "metadata": {
        "id": "9SwFzVd7sR6Y"
      },
      "id": "9SwFzVd7sR6Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
      ],
      "metadata": {
        "id": "79uSyB_KtJMQ"
      },
      "id": "79uSyB_KtJMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure pad token is set correctly\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n"
      ],
      "metadata": {
        "id": "Htpgxd0kt513"
      },
      "id": "Htpgxd0kt513",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512  # 256 to speed up training\n",
        "\n",
        "def tokenize_for_sft(example):\n",
        "    text = example[\"prompt\"] + \" \" + example[\"completion\"]\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=False,   # IMPORTANT: no padding here, collator will handle it per-batch\n",
        "    )\n",
        "    return tokens\n",
        "\n",
        "train_tok = train_ds.map(tokenize_for_sft, remove_columns=train_ds.column_names)\n",
        "val_tok   = val_ds.map(tokenize_for_sft, remove_columns=val_ds.column_names)\n",
        "\n",
        "print(\"‚úÖ Tokenization done\")\n",
        "print(train_tok[0].keys())\n"
      ],
      "metadata": {
        "id": "kWt5qQYvt8pN"
      },
      "id": "kWt5qQYvt8pN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def causal_lm_collator(features):\n",
        "    # Pad inputs dynamically to the longest sequence in the batch\n",
        "    batch = tokenizer.pad(\n",
        "        features,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Create labels from input_ids\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Ignore loss on padding tokens\n",
        "    labels[batch[\"attention_mask\"] == 0] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch\n",
        "\n",
        "data_collator = causal_lm_collator\n",
        "print(\"‚úÖ Custom collator ready\")\n"
      ],
      "metadata": {
        "id": "VVd8nQfOt_q8"
      },
      "id": "VVd8nQfOt_q8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from trl import SFTTrainer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class SFTTrainerWithSampler(SFTTrainer):\n",
        "    def get_train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.args.train_batch_size,\n",
        "            sampler=sampler,\n",
        "            collate_fn=self.data_collator,  # <-- uses our custom collator now\n",
        "            drop_last=self.args.dataloader_drop_last,\n",
        "            num_workers=self.args.dataloader_num_workers,\n",
        "            pin_memory=self.args.dataloader_pin_memory,\n",
        "        )\n",
        "\n",
        "trainer_params = set(inspect.signature(SFTTrainer.__init__).parameters.keys())\n",
        "\n",
        "base_kwargs = {\n",
        "    \"model\": model,\n",
        "    \"train_dataset\": train_tok,\n",
        "    \"eval_dataset\": val_tok,\n",
        "    \"args\": training_args,\n",
        "    \"peft_config\": lora_config,\n",
        "    \"data_collator\": data_collator,  # <-- IMPORTANT\n",
        "}\n",
        "\n",
        "if \"tokenizer\" in trainer_params:\n",
        "    base_kwargs[\"tokenizer\"] = tokenizer\n",
        "elif \"processing_class\" in trainer_params:\n",
        "    base_kwargs[\"processing_class\"] = tokenizer\n",
        "\n",
        "trainer_kwargs = {k: v for k, v in base_kwargs.items() if k in trainer_params}\n",
        "\n",
        "print(\"Passing to SFTTrainer:\", sorted(trainer_kwargs.keys()))\n",
        "\n",
        "trainer = SFTTrainerWithSampler(**trainer_kwargs)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"‚úÖ Training done. global_step:\", trainer.state.global_step)\n"
      ],
      "metadata": {
        "id": "8yxPvKlIuDAO"
      },
      "id": "8yxPvKlIuDAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS3cHY2Mp6tB"
      },
      "source": [
        "## 9) Evaluate (Validation + Test)\n",
        "We generate a short output and parse it as `not` or `offensive`.\n"
      ],
      "id": "MS3cHY2Mp6tB"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_label_generate(tweet: str, max_new_tokens=3) -> str:\n",
        "    prompt = build_prompt(tweet)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    ans = decoded.split(\"Answer:\")[-1].strip().lower()\n",
        "    first = ans.split()[0] if ans else \"\"\n",
        "\n",
        "    if \"off\" in first:\n",
        "        return \"offensive\"\n",
        "    return \"not\"\n",
        "\n",
        "def eval_split_generate(df_part, name=\"Split\"):\n",
        "    y_true = df_part[\"label\"].tolist()\n",
        "    y_pred = [predict_label_generate(t) for t in df_part[\"tweet\"].astype(str).tolist()]\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1  = f1_score(y_true, y_pred, pos_label=\"offensive\")\n",
        "\n",
        "    print(f\"\\n==== {name} (generate) ====\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"F1 (offensive positive):\", f1)\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "eval_split_generate(val_part, \"Validation\")\n",
        "eval_split_generate(test_part, \"Test\")\n"
      ],
      "metadata": {
        "id": "LSNISB-FF3EW"
      },
      "id": "LSNISB-FF3EW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "def eval_split_generate_return(df_part, name=\"Split\"):\n",
        "    y_true = df_part[\"label\"].tolist()\n",
        "    y_pred = [predict_label_generate(t) for t in df_part[\"tweet\"].astype(str).tolist()]\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1  = f1_score(y_true, y_pred, pos_label=\"offensive\")\n",
        "    report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
        "\n",
        "    return {\n",
        "        \"split\": name,\n",
        "        \"accuracy\": acc,\n",
        "        \"f1_offensive\": f1,\n",
        "        \"precision_not\": report[\"not\"][\"precision\"],\n",
        "        \"recall_not\": report[\"not\"][\"recall\"],\n",
        "        \"f1_not\": report[\"not\"][\"f1-score\"],\n",
        "        \"precision_offensive\": report[\"offensive\"][\"precision\"],\n",
        "        \"recall_offensive\": report[\"offensive\"][\"recall\"],\n",
        "        \"f1_offensive_class\": report[\"offensive\"][\"f1-score\"],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Am-sz2WGLogs"
      },
      "id": "Am-sz2WGLogs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "results.append(eval_split_generate_return(val_part, \"Validation\"))\n",
        "results.append(eval_split_generate_return(test_part, \"Test\"))\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "_YL5EzVRL4Jw"
      },
      "id": "_YL5EzVRL4Jw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = \"/content/generate_evaluation_results.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "print(\"‚úÖ Saved generate results to:\", out_path)\n"
      ],
      "metadata": {
        "id": "jUwmZcA0L8z4"
      },
      "id": "jUwmZcA0L8z4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def save_generate_report(df_part, name):\n",
        "    y_true = df_part[\"label\"].tolist()\n",
        "    y_pred = [predict_label_generate(t) for t in df_part[\"tweet\"].astype(str).tolist()]\n",
        "    report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
        "\n",
        "    path = f\"/content/{name.lower()}_generate_report.json\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    print(\"‚úÖ Saved:\", path)\n",
        "\n",
        "save_generate_report(val_part, \"Validation\")\n",
        "save_generate_report(test_part, \"Test\")\n"
      ],
      "metadata": {
        "id": "DHxj51ezMBbW"
      },
      "id": "DHxj51ezMBbW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK00_wCJp6tB"
      },
      "source": [
        "## 10) Save Adapter + Predict Unlabeled Dataset A\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "id": "xK00_wCJp6tB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLepE__yp6tB"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "save_dir = \"/content/qwen3_offensive_lora_adapter\"\n",
        "trainer.model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "print(\"‚úÖ Adapter saved to:\", save_dir)\n"
      ],
      "id": "LLepE__yp6tB"
    },
    {
      "cell_type": "code",
      "source": [
        "real_df_out = real_df.copy()\n",
        "real_df_out[\"pred_label\"] = real_df_out[\"text\"].astype(str).apply(predict_label_generate)\n",
        "real_df_out[\"pred_label_id\"] = real_df_out[\"pred_label\"].map({\"not\": 0, \"offensive\": 1})\n",
        "\n",
        "out_path = \"/content/merged_twitterdata_with_preds.csv\"\n",
        "real_df_out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"‚úÖ Predictions saved to:\", out_path)\n",
        "real_df_out.head()\n"
      ],
      "metadata": {
        "id": "vCEa9ajxq7dL"
      },
      "id": "vCEa9ajxq7dL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_df_out[\"pred_label\"].value_counts(normalize=True) * 100\n"
      ],
      "metadata": {
        "id": "nQPJQjPPTCEu"
      },
      "id": "nQPJQjPPTCEu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_df_out[\"pred_label\"].value_counts()\n"
      ],
      "metadata": {
        "id": "B2uoUD-4TCtj"
      },
      "id": "B2uoUD-4TCtj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_df_out.sample(10)[[\"text\", \"pred_label\"]]\n"
      ],
      "metadata": {
        "id": "02dtDeDjTFbX"
      },
      "id": "02dtDeDjTFbX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After enhancement prompt"
      ],
      "metadata": {
        "id": "oWYU8aKCe_MF"
      },
      "id": "oWYU8aKCe_MF"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(tweet):\n",
        "    return f\"\"\"\n",
        "You are an Arabic content moderation system.\n",
        "Classify the following tweet as \"offensive\" if it contains insults, profanity, or abusive language,\n",
        "even if it appears within a discussion or argument.\n",
        "Otherwise, classify it as \"not\".\n",
        "\n",
        "Tweet:\n",
        "{tweet}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "TeDiv_sITIJ2"
      },
      "id": "TeDiv_sITIJ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_df_out[\"pred_label\"].value_counts(normalize=True) * 100\n"
      ],
      "metadata": {
        "id": "6tHKN427WO6D"
      },
      "id": "6tHKN427WO6D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n"
      ],
      "metadata": {
        "id": "b-4UXUABbhak"
      },
      "id": "b-4UXUABbhak",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r qwen3_offensive_lora_adapter.zip qwen3_offensive_lora_adapter\n"
      ],
      "metadata": {
        "id": "e0f6HvnIbQmF"
      },
      "id": "e0f6HvnIbQmF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"qwen3_offensive_lora_adapter.zip\")\n"
      ],
      "metadata": {
        "id": "aWSKc3BkcZkt"
      },
      "id": "aWSKc3BkcZkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Live Stream\n"
      ],
      "metadata": {
        "id": "qCU9T4fke3AD"
      },
      "id": "qCU9T4fke3AD"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_label_generate(text: str) -> str:\n",
        "    prompt = build_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=3,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    ans = decoded.split(\"Answer:\")[-1].strip().lower()\n",
        "    first = ans.split()[0] if ans else \"\"\n",
        "\n",
        "    return \"offensive\" if \"off\" in first else \"not\"\n"
      ],
      "metadata": {
        "id": "2PgXghsncmVZ"
      },
      "id": "2PgXghsncmVZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüõ°Ô∏è Arabic Offensive Detector (LIVE)\")\n",
        "print(\"ÿßŸÉÿ™ÿ® ŸÜÿµ ÿπÿ±ÿ®Ÿä Ÿàÿßÿ∂ÿ∫ÿ∑ Enter\")\n",
        "print(\"ÿßŸÉÿ™ÿ® exit ŸÑŸÑÿÆÿ±Ÿàÿ¨\\n\")\n",
        "\n",
        "while True:\n",
        "    text = input(\"üìù ÿßŸÑŸÜÿµ: \").strip()\n",
        "    if text.lower() == \"exit\":\n",
        "        print(\"üëã ÿ™ŸÖ ÿßŸÑÿÆÿ±Ÿàÿ¨\")\n",
        "        break\n",
        "\n",
        "    label = predict_label_generate(text)\n",
        "\n",
        "    if label == \"offensive\":\n",
        "        print(\"üö´ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©: OFFENSIVE\\n\")\n",
        "    else:\n",
        "        print(\"‚úÖ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©: NOT\\n\")\n"
      ],
      "metadata": {
        "id": "ofE7bWchdiq7"
      },
      "id": "ofE7bWchdiq7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhancement - The final system adopts a hybrid approach, combining a fine-tuned LLM with a lightweight rule-based layer to capture explicit profanity that may be underrepresented in the training data."
      ],
      "metadata": {
        "id": "TC0PdmVRgjo4"
      },
      "id": "TC0PdmVRgjo4"
    },
    {
      "cell_type": "code",
      "source": [
        "BAD_WORDS = [\n",
        "    \"ÿ≠ŸÖÿßÿ±\", \"ÿ≠ŸÖŸäÿ±\", \"Ÿäÿß ÿ≠ŸÖÿßÿ±\",\n",
        "    \"ŸÉŸÑÿ®\", \"Ÿäÿß ŸÉŸÑÿ®\",\n",
        "    \"ÿ≠ŸäŸàÿßŸÜ\", \"Ÿäÿß ÿ≠ŸäŸàÿßŸÜ\",\n",
        "    \"ÿ∫ÿ®Ÿä\", \"ÿßŸáÿ®ŸÑ\", \"ŸÇÿ∞ÿ±\", \"Ÿàÿ≥ÿÆ\",\n",
        "    \"ŸÑÿπŸÜÿ©\", \"ŸÑÿπŸÜ\", \"ŸäŸÑÿπŸÜ\", \"ÿ™ÿ®ÿßŸã\", \"ÿ™ŸÅŸà\"\n",
        "]\n",
        "\n",
        "def contains_bad_words(text: str) -> bool:\n",
        "    t = text.replace(\"ŸÄ\", \"\").strip().lower()\n",
        "    return any(w in t for w in BAD_WORDS)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_label_generate_strict(text: str) -> str:\n",
        "    # 1) Hard rule first (fast + catches obvious insults)\n",
        "    if contains_bad_words(text):\n",
        "        return \"offensive\"\n",
        "    # 2) Otherwise fall back to model\n",
        "    return predict_label_generate(text)\n"
      ],
      "metadata": {
        "id": "zQaJMTwNeMR5"
      },
      "id": "zQaJMTwNeMR5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = predict_label_generate_strict(text)\n"
      ],
      "metadata": {
        "id": "I-rJKanxfl8m"
      },
      "id": "I-rJKanxfl8m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(tweet: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are an Arabic content moderation system.\n",
        "Classify as \"offensive\" if the text contains ANY insult or profanity (e.g., ÿ≠ŸÖÿßÿ±ÿå ŸÉŸÑÿ®ÿå ÿ∫ÿ®Ÿäÿå ÿßŸáÿ®ŸÑ) or abusive language,\n",
        "even if used casually or in an argument.\n",
        "Otherwise classify as \"not\".\n",
        "\n",
        "Tweet:\n",
        "{tweet}\n",
        "\n",
        "Answer:\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "Z0PkyLaNfqjT"
      },
      "id": "Z0PkyLaNfqjT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# Prompt (STRICT)\n",
        "# -----------------------------\n",
        "def build_prompt(tweet: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are an Arabic content moderation system.\n",
        "Classify the following text as \"offensive\" if it contains insults, profanity, or abusive language,\n",
        "even if it appears within a discussion or argument. Otherwise, classify it as \"not\".\n",
        "\n",
        "Text:\n",
        "{tweet}\n",
        "\n",
        "Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "# -----------------------------\n",
        "# Inference (generate-based) - matches your training/eval\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def predict_label_generate(text: str, max_new_tokens: int = 3) -> str:\n",
        "    prompt = build_prompt(text)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    ans = decoded.split(\"Answer:\")[-1].strip().lower()\n",
        "    first = ans.split()[0] if ans else \"\"\n",
        "\n",
        "    return \"offensive\" if \"off\" in first else \"not\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# LIVE CLI (no saving)\n",
        "# -----------------------------\n",
        "print(\"\\nüõ°Ô∏è Arabic Offensive Detector (LIVE)\")\n",
        "print(\"ÿßŸÉÿ™ÿ® ŸÜÿµ ÿπÿ±ÿ®Ÿä Ÿàÿßÿ∂ÿ∫ÿ∑ Enter\")\n",
        "print(\"ÿßŸÉÿ™ÿ® exit ŸÑŸÑÿÆÿ±Ÿàÿ¨\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        text = input(\"üìù ÿßŸÑŸÜÿµ: \").strip()\n",
        "        if not text:\n",
        "            print(\"‚ö†Ô∏è ÿßŸÉÿ™ÿ® ŸÜÿµ ÿ∫Ÿäÿ± ŸÅÿßÿ∂Ÿä.\\n\")\n",
        "            continue\n",
        "\n",
        "        if text.lower() in [\"exit\", \"quit\", \"q\"]:\n",
        "            print(\"üëã ÿ™ŸÖ ÿßŸÑÿÆÿ±Ÿàÿ¨\")\n",
        "            break\n",
        "\n",
        "        t0 = time.time()\n",
        "        label = predict_label_generate(text)\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        if label == \"offensive\":\n",
        "            print(f\"üö´ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©: OFFENSIVE   |  ‚è±Ô∏è {dt:.2f}s\\n\")\n",
        "        else:\n",
        "            print(f\"‚úÖ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©: NOT         |  ‚è±Ô∏è {dt:.2f}s\\n\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüëã ÿ™ŸÖ ÿßŸÑÿÆÿ±Ÿàÿ¨ (Ctrl+C)\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\\n\")\n"
      ],
      "metadata": {
        "id": "-mtdVsD7fuLT"
      },
      "id": "-mtdVsD7fuLT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------\n",
        "# Prompt (STRICT)\n",
        "# -----------------------------\n",
        "def build_prompt(tweet: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are an Arabic content moderation system.\n",
        "Classify the following text as \"offensive\" if it contains insults, profanity, or abusive language,\n",
        "even if it appears within a discussion or argument. Otherwise, classify it as \"not\".\n",
        "\n",
        "Text:\n",
        "{tweet}\n",
        "\n",
        "Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "# -----------------------------\n",
        "# Inference (generate-based)\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def predict_label_generate(text: str, max_new_tokens: int = 3) -> str:\n",
        "    prompt = build_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    ans = decoded.split(\"Answer:\")[-1].strip().lower()\n",
        "    first = ans.split()[0] if ans else \"\"\n",
        "    return \"offensive\" if \"off\" in first else \"not\"\n",
        "\n",
        "# -----------------------------\n",
        "# Confidence (logits of next token: \"not\" vs \"offensive\")\n",
        "# -----------------------------\n",
        "def _get_single_token_id(tokenizer, variants):\n",
        "    for v in variants:\n",
        "        ids = tokenizer.encode(v, add_special_tokens=False)\n",
        "        if len(ids) == 1:\n",
        "            return ids[0]\n",
        "    return tokenizer.encode(variants[0], add_special_tokens=False)[0]\n",
        "\n",
        "NOT_ID = _get_single_token_id(tokenizer, [\"not\", \" not\"])\n",
        "OFF_ID = _get_single_token_id(tokenizer, [\"offensive\", \" offensive\"])\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_with_confidence(text: str):\n",
        "    prompt = build_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits[0, -1, :]  # next-token logits\n",
        "\n",
        "    two = torch.stack([logits[NOT_ID], logits[OFF_ID]], dim=0)\n",
        "    probs = F.softmax(two, dim=0)\n",
        "    p_not = probs[0].item()\n",
        "    p_off = probs[1].item()\n",
        "\n",
        "    pred = \"offensive\" if p_off > p_not else \"not\"\n",
        "    conf = max(p_off, p_not)\n",
        "    return pred, conf\n",
        "\n",
        "# -----------------------------\n",
        "# UI Card (Colors) + Confidence Rate\n",
        "# -----------------------------\n",
        "def render_card(user_text: str):\n",
        "    user_text = (user_text or \"\").strip()\n",
        "    if not user_text:\n",
        "        return \"\"\"\n",
        "        <div class=\"card neutral\">\n",
        "            <div class=\"title\">‚Äî</div>\n",
        "            <div class=\"sub\">Confidence: ‚Äî</div>\n",
        "            <div class=\"text\">ÿßŸÉÿ™ÿ® ŸÜÿµ ÿ´ŸÖ ÿßÿ∂ÿ∫ÿ∑ Submit</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    pred, conf = predict_with_confidence(user_text)\n",
        "    conf_pct = f\"{conf*100:.2f}%\"\n",
        "\n",
        "    if pred == \"offensive\":\n",
        "        # RED\n",
        "        return f\"\"\"\n",
        "        <div class=\"card bad\">\n",
        "            <div class=\"title\"><span class=\"dot\"></span> OFFENSIVE</div>\n",
        "            <div class=\"sub\">Confidence: {conf_pct}</div>\n",
        "            <div class=\"text\">{user_text}</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # GREEN\n",
        "        return f\"\"\"\n",
        "        <div class=\"card good\">\n",
        "            <div class=\"title\"><span class=\"dot\"></span> NOT</div>\n",
        "            <div class=\"sub\">Confidence: {conf_pct}</div>\n",
        "            <div class=\"text\">{user_text}</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "# -----------------------------\n",
        "# CSS\n",
        "# -----------------------------\n",
        "CSS = \"\"\"\n",
        ".wrap {max-width: 1000px; margin: 0 auto;}\n",
        ".card{\n",
        "  border-radius: 12px;\n",
        "  padding: 14px 16px;\n",
        "  border: 1px solid rgba(0,0,0,0.10);\n",
        "  box-shadow: 0 6px 14px rgba(0,0,0,0.08);\n",
        "  direction: rtl;\n",
        "  font-family: Arial, sans-serif;\n",
        "  min-height: 150px;\n",
        "}\n",
        ".title{ display:flex; align-items:center; gap:10px; font-size: 26px; font-weight: 800; }\n",
        ".sub{ margin-top: 6px; font-size: 16px; opacity: 0.9; }\n",
        ".text{\n",
        "  margin-top: 12px; font-size: 18px; line-height: 1.6;\n",
        "  background: rgba(255,255,255,0.40);\n",
        "  padding: 10px 12px; border-radius: 10px;\n",
        "}\n",
        ".dot{ width: 14px; height: 14px; border-radius: 50%; display:inline-block; }\n",
        "\n",
        ".good{ background:#dff3df; color:#0f3d0f; }\n",
        ".good .dot{ background:#1f8f1f; }\n",
        "\n",
        ".bad{ background:#ffd7d7; color:#5a0b0b; }\n",
        ".bad .dot{ background:#d11a1a; }\n",
        "\n",
        ".neutral{ background:#f2f2f2; color:#222; }\n",
        ".neutral .dot{ background:#999; }\n",
        "\n",
        "/* Buttons */\n",
        "button.primary { background: #d77219 !important; border: none !important; }\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI: 2 Textboxes + 2 Submit (separate outputs) + Exit button\n",
        "# -----------------------------\n",
        "with gr.Blocks(css=CSS) as demo:\n",
        "    gr.Markdown(\"## üõ°Ô∏è Arabic Offensive Detector\", elem_classes=\"wrap\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # -------- Textbox 1 --------\n",
        "        with gr.Column(scale=3):\n",
        "            user_text_1 = gr.Textbox(label=\"ÿßŸÑŸÜÿµ (1)\", placeholder=\"ÿßŸÉÿ™ÿ® ÿßŸÑŸÜÿµ ÿßŸÑÿ£ŸàŸÑ ŸáŸÜÿß...\", lines=6)\n",
        "            with gr.Row():\n",
        "                submit_btn_1 = gr.Button(\"Submit 1\", variant=\"primary\")\n",
        "        with gr.Column(scale=2):\n",
        "            out_html_1 = gr.HTML(render_card(\"\"))\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # -------- Textbox 2 --------\n",
        "        with gr.Column(scale=3):\n",
        "            user_text_2 = gr.Textbox(label=\"ÿßŸÑŸÜÿµ (2)\", placeholder=\"ÿßŸÉÿ™ÿ® ÿßŸÑŸÜÿµ ÿßŸÑÿ´ÿßŸÜŸä ŸáŸÜÿß...\", lines=6)\n",
        "            with gr.Row():\n",
        "                submit_btn_2 = gr.Button(\"Submit 2\", variant=\"primary\")\n",
        "        with gr.Column(scale=2):\n",
        "            out_html_2 = gr.HTML(render_card(\"\"))\n",
        "\n",
        "    with gr.Row():\n",
        "        clear_btn = gr.Button(\"Clear\")\n",
        "        exit_btn = gr.Button(\"Exit\")\n",
        "\n",
        "    # Submit actions (separate)\n",
        "    submit_btn_1.click(fn=render_card, inputs=user_text_1, outputs=out_html_1)\n",
        "    user_text_1.submit(fn=render_card, inputs=user_text_1, outputs=out_html_1)\n",
        "\n",
        "    submit_btn_2.click(fn=render_card, inputs=user_text_2, outputs=out_html_2)\n",
        "    user_text_2.submit(fn=render_card, inputs=user_text_2, outputs=out_html_2)\n",
        "\n",
        "    # Clear both\n",
        "    clear_btn.click(\n",
        "        fn=lambda: (\"\", render_card(\"\"), \"\", render_card(\"\")),\n",
        "        inputs=None,\n",
        "        outputs=[user_text_1, out_html_1, user_text_2, out_html_2],\n",
        "    )\n",
        "\n",
        "    # Exit: close server process\n",
        "    def _exit():\n",
        "        raise SystemExit\n",
        "\n",
        "    exit_btn.click(fn=_exit, inputs=None, outputs=None)\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "WYkS8X5D4yS_"
      },
      "id": "WYkS8X5D4yS_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Rusult after Enhancement"
      ],
      "metadata": {
        "id": "UMhLtAH2id6V"
      },
      "id": "UMhLtAH2id6V"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, textwrap, shutil\n",
        "from datetime import datetime\n",
        "\n",
        "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "EXPORT_DIR = f\"/content/offensive_export_{RUN_TAG}\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Export folder:\", EXPORT_DIR)\n"
      ],
      "metadata": {
        "id": "a3j9JULcikJH"
      },
      "id": "a3j9JULcikJH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADAPTER_DIR = os.path.join(EXPORT_DIR, \"qwen3_offensive_lora_adapter\")\n",
        "\n",
        "trainer.model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "\n",
        "print(\"‚úÖ Adapter+Tokenizer saved:\", ADAPTER_DIR)\n",
        "print(\"Files:\", os.listdir(ADAPTER_DIR))\n"
      ],
      "metadata": {
        "id": "OvVoW8kCisAd"
      },
      "id": "OvVoW8kCisAd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Save BAD_WORDS list\n",
        "bad_words_path = os.path.join(EXPORT_DIR, \"bad_words.json\")\n",
        "with open(bad_words_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(BAD_WORDS, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# 2) Save prompt template (string)\n",
        "prompt_path = os.path.join(EXPORT_DIR, \"prompt_template.txt\")\n",
        "with open(prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(build_prompt(\"{TEXT_HERE}\"))\n",
        "\n",
        "print(\"‚úÖ Saved:\", bad_words_path)\n",
        "print(\"‚úÖ Saved:\", prompt_path)\n"
      ],
      "metadata": {
        "id": "0-P_RGPCivcS"
      },
      "id": "0-P_RGPCivcS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "results.append(eval_split_generate_return(val_part, \"Validation (strict)\"))\n",
        "results.append(eval_split_generate_return(test_part, \"Test (strict)\"))\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "csv_path = os.path.join(EXPORT_DIR, \"generate_eval_results_strict.csv\")\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"‚úÖ Saved:\", csv_path)\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "2lIaE4Nbixyb"
      },
      "id": "2lIaE4Nbixyb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def save_generate_report(df_part, name):\n",
        "    y_true = df_part[\"label\"].tolist()\n",
        "    y_pred = [predict_label_generate(t) for t in df_part[\"tweet\"].astype(str).tolist()]\n",
        "    report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
        "\n",
        "    path = os.path.join(EXPORT_DIR, f\"{name.lower().replace(' ', '_')}_generate_report_strict.json\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"‚úÖ Saved:\", path)\n",
        "\n",
        "save_generate_report(val_part, \"Validation\")\n",
        "save_generate_report(test_part, \"Test\")\n"
      ],
      "metadata": {
        "id": "QwMm20tfi0Jn"
      },
      "id": "QwMm20tfi0Jn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑÿ£ÿµŸÑŸä\n",
        "source_dir = \"/content/offensive_export_20260109_161538\"\n",
        "\n",
        "# ÿßÿ≥ŸÖ ŸÖŸÑŸÅ zip ÿßŸÑŸÜÿßÿ™ÿ¨\n",
        "zip_path = \"/content/offensive_export_20260109_161538.zip\"\n",
        "\n",
        "# ÿ•ŸÜÿ¥ÿßÿ° ŸÖŸÑŸÅ zip\n",
        "shutil.make_archive(\n",
        "    base_name=zip_path.replace(\".zip\", \"\"),\n",
        "    format=\"zip\",\n",
        "    root_dir=source_dir\n",
        ")\n",
        "\n",
        "print(\"‚úÖ ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÖÿ∂ÿ∫Ÿàÿ∑:\", zip_path)\n"
      ],
      "metadata": {
        "id": "36LuNOSM5Lyi"
      },
      "id": "36LuNOSM5Lyi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/offensive_export_20260109_161538.zip\")\n"
      ],
      "metadata": {
        "id": "sktTeeyh56lf"
      },
      "id": "sktTeeyh56lf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"/mnt/data/merged_twitterdata/human classification333.csv\"\n",
        "df = pd.read_csv(path)\n"
      ],
      "metadata": {
        "id": "MiaTUP3A6tWX"
      },
      "id": "MiaTUP3A6tWX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tr58Vn_Ep0R3"
      },
      "id": "tr58Vn_Ep0R3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model vs Human Comparison"
      ],
      "metadata": {
        "id": "PpKIRXFAp0fr"
      },
      "id": "PpKIRXFAp0fr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation on Full Human-Labeled Dataset (Unbalanced Test)"
      ],
      "metadata": {
        "id": "7nbMH4w8u97L"
      },
      "id": "7nbMH4w8u97L"
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/offensive_export_20260109_161538.zip\"      # ŸÖÿ≥ÿßÿ± ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÖÿ∂ÿ∫Ÿàÿ∑\n",
        "extract_to = \"/content/qwen3_offensive_lora_adapter\"   # ŸÖÿ¨ŸÑÿØ ÿßŸÑŸÅŸÉ\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"‚úÖ Unzip done to:\", extract_to)\n"
      ],
      "metadata": {
        "id": "abySLwlPxO8O"
      },
      "id": "abySLwlPxO8O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Exists /content ?\", os.path.exists(\"/content\"))\n",
        "print(\"List /content:\")\n",
        "print(os.listdir(\"/content\"))\n",
        "\n",
        "print(\"\\nExists adapter dir ?\", os.path.exists(\"/content/qwen3_offensive_lora_adapter\"))\n",
        "if os.path.exists(\"/content/qwen3_offensive_lora_adapter\"):\n",
        "    print(\"List adapter dir:\")\n",
        "    print(os.listdir(\"/content/qwen3_offensive_lora_adapter\"))\n"
      ],
      "metadata": {
        "id": "iLgpzzV91j3P"
      },
      "id": "iLgpzzV91j3P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "BASE_MODEL_ID = \"Qwen/Qwen3-4B\"\n",
        "LORA_ADAPTER_PATH = \"/content/qwen3_offensive_lora_adapter/qwen3_offensive_lora_adapter\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    LORA_ADAPTER_PATH\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "zPHVqaYmu8n2"
      },
      "id": "zPHVqaYmu8n2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "DATA_PATH = \"/content/merged_twitterdata with human classification333.csv\"\n",
        "\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"classification\"\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "IMBALANCE_RATIO_THRESHOLD = 1.5\n",
        "\n",
        "MAX_INPUT_TOKENS = 256\n",
        "MAX_NEW_TOKENS = 6\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# -------------------------\n",
        "# 1) Load dataset\n",
        "# -------------------------\n",
        "df = pd.read_csv(DATA_PATH, sep=\";\", encoding=\"utf-8-sig\")\n",
        "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
        "df[LABEL_COL] = df[LABEL_COL].astype(str).str.strip().str.lower()\n",
        "\n",
        "# ŸÑŸà ÿπŸÜÿØŸÉ ÿ™ÿ≥ŸÖŸäÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ©ÿå Ÿàÿ≠ŸëÿØŸáÿß ŸáŸÜÿß\n",
        "label_map = {\n",
        "    \"offensive\": \"offensive\",\n",
        "    \"not\": \"not\",\n",
        "    \"not_offensive\": \"not\",\n",
        "    \"non-offensive\": \"not\",\n",
        "    \"non_offensive\": \"not\",\n",
        "    \"0\": \"not\",\n",
        "    \"1\": \"offensive\",\n",
        "}\n",
        "df[LABEL_COL] = df[LABEL_COL].map(lambda x: label_map.get(x, x))\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nClass distribution (unbalanced):\")\n",
        "print(df[LABEL_COL].value_counts(dropna=False))\n",
        "print(\"\\nClass distribution (%):\")\n",
        "print((df[LABEL_COL].value_counts(normalize=True) * 100).round(2))\n",
        "\n",
        "# -------------------------\n",
        "# 2) Prompt + parsing\n",
        "# -------------------------\n",
        "_off_pat = re.compile(r\"\\boffensive\\b\", re.IGNORECASE)\n",
        "_not_pat = re.compile(r\"\\bnot\\b\", re.IGNORECASE)\n",
        "\n",
        "def build_prompt(text: str) -> str:\n",
        "    return (\n",
        "        \"You are a strict classifier.\\n\"\n",
        "        \"Classify the text as Offensive or Not.\\n\"\n",
        "        \"Return exactly one word: Offensive or Not.\\n\\n\"\n",
        "        f\"Text: {text}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "def parse_label(generated: str) -> str:\n",
        "    t = generated.strip().lower()\n",
        "    if _off_pat.search(t):\n",
        "        return \"offensive\"\n",
        "    if _not_pat.search(t):\n",
        "        return \"not\"\n",
        "    return \"not\"  # fallback\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_batch(texts):\n",
        "    prompts = [build_prompt(t) for t in texts]\n",
        "    inputs = tokenizer(\n",
        "        prompts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_INPUT_TOKENS,\n",
        "    ).to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "    preds = []\n",
        "    for full in decoded:\n",
        "        # ÿÆÿ∞ ÿßŸÑÿ¨ÿ≤ÿ° ÿ®ÿπÿØ Answer:\n",
        "        tail = full.split(\"Answer:\")[-1] if \"Answer:\" in full else full\n",
        "        preds.append(parse_label(tail))\n",
        "    return preds\n",
        "\n",
        "def evaluate(df_eval: pd.DataFrame, tag: str):\n",
        "    texts = df_eval[TEXT_COL].tolist()\n",
        "    y_true = df_eval[LABEL_COL].tolist()\n",
        "\n",
        "    y_pred = []\n",
        "    for i in range(0, len(texts), BATCH_SIZE):\n",
        "        y_pred.extend(predict_batch(texts[i:i+BATCH_SIZE]))\n",
        "\n",
        "    labels_sorted = sorted(list(set(y_true) | set(y_pred)))\n",
        "\n",
        "    print(f\"\\n==================== {tag} ====================\")\n",
        "    print(\"N =\", len(df_eval))\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_true, y_pred, labels=labels_sorted, digits=4))\n",
        "    print(\"\\nConfusion matrix (rows=true, cols=pred) order:\", labels_sorted)\n",
        "    print(confusion_matrix(y_true, y_pred, labels=labels_sorted))\n",
        "\n",
        "    out = df_eval[[TEXT_COL, LABEL_COL]].copy()\n",
        "    out[\"pred\"] = y_pred\n",
        "    out.to_csv(f\"eval_{tag.lower()}_results.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\nSaved: eval_{tag.lower()}_results.csv\")\n",
        "\n",
        "# -------------------------\n",
        "# 3) Unbalanced eval\n",
        "# -------------------------\n",
        "evaluate(df, \"UNBALANCED\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Balanced eval (undersampling if imbalanced)\n",
        "# -------------------------\n",
        "counts = df[LABEL_COL].value_counts()\n",
        "min_c = counts.min()\n",
        "max_c = counts.max()\n",
        "ratio = (max_c / min_c) if min_c > 0 else np.inf\n",
        "\n",
        "is_imbalanced = ratio > IMBALANCE_RATIO_THRESHOLD\n",
        "print(f\"\\nImbalance ratio (max/min) = {ratio:.3f} | threshold={IMBALANCE_RATIO_THRESHOLD} => imbalanced? {is_imbalanced}\")\n",
        "\n",
        "if is_imbalanced:\n",
        "    parts = []\n",
        "    for cls, grp in df.groupby(LABEL_COL):\n",
        "        parts.append(grp.sample(n=min_c, random_state=RANDOM_SEED))\n",
        "    df_bal = pd.concat(parts).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "else:\n",
        "    df_bal = df.copy()\n",
        "\n",
        "print(\"\\nBalanced class distribution:\")\n",
        "print(df_bal[LABEL_COL].value_counts())\n",
        "\n",
        "evaluate(df_bal, \"BALANCED\")\n"
      ],
      "metadata": {
        "id": "SXXhB9-etH37"
      },
      "id": "SXXhB9-etH37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJl5sDbj2USm"
      },
      "id": "KJl5sDbj2USm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}