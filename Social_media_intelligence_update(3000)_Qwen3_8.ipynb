{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac956dc-a3b0-4b65-b856-bfd2792f5b2c",
      "metadata": {
        "id": "5ac956dc-a3b0-4b65-b856-bfd2792f5b2c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ù…Ø«Ø§Ù„ Ù„Ùˆ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù:\n",
        "DATA_PATH = \"/content/Arabic_Reports(3000).csv\"   # Ù†ÙØ³ Ø§Ù„ÙÙˆÙ„Ø¯Ø±\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e58683-b0ed-4401-b6dc-b292723089b9",
      "metadata": {
        "id": "d4e58683-b0ed-4401-b6dc-b292723089b9"
      },
      "outputs": [],
      "source": [
        "# ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "df = df.rename(columns={\n",
        "    \"Ø§Ù„Ø¨Ù„Ø§Øº\": \"text\",\n",
        "    \"Ø§Ù„ØªØµÙ†ÙŠÙ\": \"label\"\n",
        "})\n",
        "\n",
        "df = df.dropna(subset=[\"text\", \"label\"]).copy()\n",
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0031e7ac-f41d-4198-94a1-de933db91223",
      "metadata": {
        "id": "0031e7ac-f41d-4198-94a1-de933db91223"
      },
      "outputs": [],
      "source": [
        "label_map = {\n",
        "    \"Profile Hacking Identity Theft\": 0,\n",
        "    \"EWallet Related Fraud\": 1,\n",
        "    \"Fraud CallVishing\": 2\n",
        "}\n",
        "\n",
        "id2label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
        "df[\"label_id\"].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4221b0a-7892-46af-bc2d-5c6d6adb1444",
      "metadata": {
        "id": "a4221b0a-7892-46af-bc2d-5c6d6adb1444"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6159ddf4-77c9-4649-98f7-516e9f5ffd70",
      "metadata": {
        "id": "6159ddf4-77c9-4649-98f7-516e9f5ffd70"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(\"Python exe:\", sys.executable)\n",
        "print(\"Python version:\", sys.version)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31db6061-8fb5-4f4b-a649-51de9b637863",
      "metadata": {
        "id": "31db6061-8fb5-4f4b-a649-51de9b637863"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -U pip\n",
        "!{sys.executable} -m pip install -U transformers accelerate torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476f15a1-6ee5-42c2-befd-952c49f0a19b",
      "metadata": {
        "id": "476f15a1-6ee5-42c2-befd-952c49f0a19b"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "print(\"OK âœ…\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4924a134-6e27-48de-a335-6daad36968ab",
      "metadata": {
        "id": "4924a134-6e27-48de-a335-6daad36968ab"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip uninstall -y torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e7443e-6dcc-4a20-a04a-78346cf2c164",
      "metadata": {
        "id": "d6e7443e-6dcc-4a20-a04a-78346cf2c164"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda in torch:\", torch.version.cuda)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31989951-99ba-447f-8305-793a2e9465be",
      "metadata": {
        "id": "31989951-99ba-447f-8305-793a2e9465be"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# IMPORTANT: reinstall both torch and torchvision from the SAME cu128 wheel index\n",
        "!{sys.executable} -m pip install -U --force-reinstall --no-cache-dir \\\n",
        "  torch torchvision --index-url https://download.pytorch.org/whl/cu128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f52872-62aa-4675-9145-c53a810d5e24",
      "metadata": {
        "id": "19f52872-62aa-4675-9145-c53a810d5e24"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip uninstall -y torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8d411c-8b46-402c-aeab-1362b802882c",
      "metadata": {
        "id": "dc8d411c-8b46-402c-aeab-1362b802882c"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -U --force-reinstall --no-cache-dir \"pillow==11.0.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36c0d67-b4ed-42a5-9380-5bd5778a1883",
      "metadata": {
        "id": "c36c0d67-b4ed-42a5-9380-5bd5778a1883"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "print(\"Pillow:\", PIL.__version__)\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torchvision:\", torchvision.__version__)\n",
        "print(\"cuda:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94b4a63-25ed-45db-bc35-04c741b37651",
      "metadata": {
        "id": "d94b4a63-25ed-45db-bc35-04c741b37651"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"Qwen/Qwen3-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"Loaded:\", model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956b796e-e843-4d33-b338-fa2dac8856e1",
      "metadata": {
        "id": "956b796e-e843-4d33-b338-fa2dac8856e1"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Inference + Show + Save\n",
        "# (Model already loaded)\n",
        "# ============================\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from IPython.display import display\n",
        "\n",
        "# ---------- 1) Prepare Data ----------\n",
        "df = df.rename(columns={\"Ø§Ù„Ø¨Ù„Ø§Øº\": \"text\", \"Ø§Ù„ØªØµÙ†ÙŠÙ\": \"label\"}).copy()\n",
        "df = df.dropna(subset=[\"text\", \"label\"]).copy()\n",
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
        "\n",
        "# ---------- 2) Labels ----------\n",
        "LABELS = [\n",
        "    \"Profile Hacking Identity Theft\",\n",
        "    \"EWallet Related Fraud\",\n",
        "    \"Fraud CallVishing\"\n",
        "]\n",
        "label_map = {lab: i for i, lab in enumerate(LABELS)}\n",
        "\n",
        "# ---------- 3) Classification Function ----------\n",
        "def classify(text, max_new_tokens=12):\n",
        "    prompt = f\"\"\"\n",
        "Task: Text Classification\n",
        "\n",
        "Choose ONLY ONE label:\n",
        "- {LABELS[0]}\n",
        "- {LABELS[1]}\n",
        "- {LABELS[2]}\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the label text.\n",
        "- No explanation.\n",
        "\n",
        "Report:\n",
        "{text}\n",
        "\n",
        "Label:\n",
        "\"\"\".strip()\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = decoded.split(\"Label:\")[-1].strip().splitlines()[0]\n",
        "\n",
        "    for lab in LABELS:\n",
        "        if lab.lower() in pred.lower():\n",
        "            return lab\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "# ---------- 4) Run Inference ----------\n",
        "df_run = df.copy()\n",
        "df_run[\"pred_label\"] = [classify(t) for t in df_run[\"text\"]]\n",
        "df_run[\"pred_id\"]   = df_run[\"pred_label\"].map(label_map)\n",
        "df_run[\"true_id\"]   = df_run[\"label\"].map(label_map)\n",
        "\n",
        "# ---------- 5) DISPLAY RESULTS (Ù‚Ø¨Ù„ Ø§Ù„Ø­ÙØ¸) ----------\n",
        "print(\"ğŸ“Š Sample of results (first 20 rows):\")\n",
        "display(df_run.head(20))\n",
        "\n",
        "# ---------- 6) Evaluation ----------\n",
        "eval_df = df_run.dropna(subset=[\"true_id\",\"pred_id\"]).copy()\n",
        "\n",
        "acc = accuracy_score(eval_df[\"true_id\"], eval_df[\"pred_id\"])\n",
        "mf1 = f1_score(eval_df[\"true_id\"], eval_df[\"pred_id\"], average=\"macro\")\n",
        "\n",
        "print(f\"\\nAccuracy: {acc:.4f}\")\n",
        "print(f\"Macro F1:  {mf1:.4f}\\n\")\n",
        "\n",
        "print(classification_report(\n",
        "    eval_df[\"true_id\"],\n",
        "    eval_df[\"pred_id\"],\n",
        "    target_names=LABELS,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# ---------- 7) Save ----------\n",
        "out_path = \"qwen3_8b_inference_results.csv\"\n",
        "df_run.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"ğŸ’¾ Saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865f92b5-ab55-40af-8350-03816db134e0",
      "metadata": {
        "id": "865f92b5-ab55-40af-8350-03816db134e0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù\n",
        "df_kw = pd.read_csv(\"Arabic_Reports(3000).csv\")\n",
        "\n",
        "# ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "df_kw = df_kw.rename(columns={\"Ø§Ù„Ø¨Ù„Ø§Øº\": \"text\", \"Ø§Ù„ØªØµÙ†ÙŠÙ\": \"label\"})\n",
        "df_kw = df_kw.dropna(subset=[\"text\", \"label\"]).copy()\n",
        "\n",
        "# Ù†Ø±ÙƒÙ‘Ø² ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø§Ù„Ù…Ø§Ù„ÙŠ\n",
        "ewallet_df = df_kw[df_kw[\"label\"] == \"EWallet Related Fraud\"]\n",
        "\n",
        "# TF-IDF Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ù‡Ù…\n",
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    max_features=30,\n",
        "    stop_words=None\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(ewallet_df[\"text\"])\n",
        "keywords = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"ğŸ”‘ Financial Fraud Keywords:\")\n",
        "print(keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285ebb32-bd6b-4b92-abb9-6f0fe4409b3c",
      "metadata": {
        "id": "285ebb32-bd6b-4b92-abb9-6f0fe4409b3c"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Inference + Eval + Show + Save\n",
        "# (Model already loaded)\n",
        "# Choose prompt_mode: \"balanced\" or \"fewshot\"\n",
        "# ============================\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from IPython.display import display\n",
        "\n",
        "# ---------- Settings ----------\n",
        "prompt_mode = \"fewshot\"   # \"balanced\" or \"fewshot\"\n",
        "N = 300                   # Ø¬Ø±Ù‘Ø¨ÙŠ 300 Ø«Ù… Ø®Ù„ÙŠÙ‡Ø§ len(df)\n",
        "max_new_tokens = 8        # label ÙÙ‚Ø·\n",
        "\n",
        "# ---------- 1) Prepare Data ----------\n",
        "df = df.rename(columns={\"Ø§Ù„Ø¨Ù„Ø§Øº\": \"text\", \"Ø§Ù„ØªØµÙ†ÙŠÙ\": \"label\"}).copy()\n",
        "df = df.dropna(subset=[\"text\", \"label\"]).copy()\n",
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
        "\n",
        "# ---------- 2) Labels ----------\n",
        "LABELS = [\n",
        "    \"Profile Hacking Identity Theft\",\n",
        "    \"EWallet Related Fraud\",\n",
        "    \"Fraud CallVishing\"\n",
        "]\n",
        "label_map = {lab: i for i, lab in enumerate(LABELS)}\n",
        "\n",
        "# ---------- 3) Prompts ----------\n",
        "BALANCED_PROMPT = f\"\"\"\n",
        "Task: Classify the Arabic report into EXACTLY ONE label.\n",
        "\n",
        "Label A) {LABELS[0]}\n",
        "Use when the report is about account hacking/compromise or identity theft:\n",
        "- hacked account, account takeover, password reset, impersonation\n",
        "- social media account hijack, profile access, stolen credentials\n",
        "\n",
        "Label B) {LABELS[1]}\n",
        "Use when the report is about financial fraud or unauthorized money movement:\n",
        "- money deducted/withdrawn/transfer from account (Ù…Ù† Ø­Ø³Ø§Ø¨ÙŠ / Ø®ØµÙ… / ØªØ­ÙˆÙŠÙ„)\n",
        "- balance affected, amount stolen (Ù…Ø¨Ù„Øº/Ø§Ù„Ù…Ø¨Ù„Øº/Ø§Ù„Ù…Ø§Ù„/Ø£Ù…ÙˆØ§Ù„ÙŠ)\n",
        "- payment/transfer via app or e-wallet services, payment links\n",
        "- sharing payment/account numbers (Ø±Ù‚Ù…) in the context of transactions\n",
        "\n",
        "Label C) {LABELS[2]}\n",
        "Use when the report is phone-call based scam (vishing):\n",
        "- scam call, impersonation via phone, caller asks for OTP/personal data\n",
        "- â€œØ§ØªØµÙ„ÙˆØ§ Ø¹Ù„ÙŠ / Ù…ÙƒØ§Ù„Ù…Ø© / Ø±Ù‚Ù…â€ with social engineering by phone\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the label text exactly as written.\n",
        "- No explanation.\n",
        "\n",
        "Report:\n",
        "{{text}}\n",
        "\n",
        "Label:\n",
        "\"\"\".strip()\n",
        "\n",
        "FEWSHOT_PROMPT = f\"\"\"\n",
        "Task: Classify the Arabic report into EXACTLY ONE label from:\n",
        "- {LABELS[0]}\n",
        "- {LABELS[1]}\n",
        "- {LABELS[2]}\n",
        "\n",
        "Examples:\n",
        "\n",
        "Report: ØªÙ… Ø§Ø®ØªØ±Ø§Ù‚ Ø­Ø³Ø§Ø¨ÙŠ ÙÙŠ Ø§Ù†Ø³ØªÙ‚Ø±Ø§Ù… ÙˆØªØºÙŠÙŠØ± ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± ÙˆØ¯Ø®Ù„ Ø´Ø®Øµ ÙŠÙ†ØªØ­Ù„ Ø´Ø®ØµÙŠØªÙŠ.\n",
        "Label: {LABELS[0]}\n",
        "\n",
        "Report: ØªÙ… Ø®ØµÙ… Ù…Ø¨Ù„Øº Ù…Ù† Ø­Ø³Ø§Ø¨ÙŠ Ø¹Ø¨Ø± ØªØ·Ø¨ÙŠÙ‚ Ø¯ÙØ¹ ÙˆØªØ­ÙˆÙŠÙ„ Ø¨Ø¯ÙˆÙ† Ø¥Ø°Ù†ÙŠ ÙˆØªØ¶Ø±Ø± Ø±ØµÙŠØ¯ÙŠ.\n",
        "Label: {LABELS[1]}\n",
        "\n",
        "Report: Ø§ØªØµÙ„ Ø¹Ù„ÙŠ Ø´Ø®Øµ ÙŠØ¯Ù‘Ø¹ÙŠ Ø£Ù†Ù‡ Ù…Ù† Ø§Ù„Ø¨Ù†Ùƒ ÙˆØ·Ù„Ø¨ Ø±Ù…Ø² Ø§Ù„ØªØ­Ù‚Ù‚ OTP ÙˆØ£ØµØ± Ø£Ø¹Ø·ÙŠÙ‡ Ø¨ÙŠØ§Ù†Ø§ØªÙŠ.\n",
        "Label: {LABELS[2]}\n",
        "\n",
        "Now classify this report.\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the label text exactly as written.\n",
        "- No explanation.\n",
        "\n",
        "Report:\n",
        "{{text}}\n",
        "\n",
        "Label:\n",
        "\"\"\".strip()\n",
        "\n",
        "def build_prompt(text: str) -> str:\n",
        "    if prompt_mode == \"balanced\":\n",
        "        return BALANCED_PROMPT.format(text=text)\n",
        "    elif prompt_mode == \"fewshot\":\n",
        "        return FEWSHOT_PROMPT.format(text=text)\n",
        "    else:\n",
        "        raise ValueError(\"prompt_mode must be 'balanced' or 'fewshot'\")\n",
        "\n",
        "# ---------- 4) Classifier ----------\n",
        "def classify(text: str) -> str:\n",
        "    prompt = build_prompt(text)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = decoded.split(\"Label:\")[-1].strip().splitlines()[0].strip()\n",
        "\n",
        "    # enforce allowed labels\n",
        "    for lab in LABELS:\n",
        "        if lab.lower() in pred.lower():\n",
        "            return lab\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "# ---------- 5) Run ----------\n",
        "df_run = df.iloc[:N].copy()\n",
        "df_run[\"pred_label\"] = [classify(t) for t in df_run[\"text\"]]\n",
        "df_run[\"pred_id\"] = df_run[\"pred_label\"].map(label_map)\n",
        "df_run[\"true_id\"] = df_run[\"label\"].map(label_map)\n",
        "\n",
        "# Show distribution (THIS IS IMPORTANT)\n",
        "print(\"True label distribution:\")\n",
        "print(df_run[\"label\"].value_counts(), \"\\n\")\n",
        "\n",
        "print(\"Pred label distribution:\")\n",
        "print(df_run[\"pred_label\"].value_counts(), \"\\n\")\n",
        "\n",
        "# ---------- 6) Display sample ----------\n",
        "print(\"ğŸ“Š Sample results:\")\n",
        "display(df_run[[\"text\", \"label\", \"pred_label\"]].head(20))\n",
        "\n",
        "# ---------- 7) Eval ----------\n",
        "eval_df = df_run.dropna(subset=[\"true_id\",\"pred_id\"]).copy()\n",
        "\n",
        "acc = accuracy_score(eval_df[\"true_id\"], eval_df[\"pred_id\"])\n",
        "mf1 = f1_score(eval_df[\"true_id\"], eval_df[\"pred_id\"], average=\"macro\")\n",
        "\n",
        "print(f\"\\nPrompt mode: {prompt_mode}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Macro F1:  {mf1:.4f}\\n\")\n",
        "\n",
        "print(classification_report(\n",
        "    eval_df[\"true_id\"], eval_df[\"pred_id\"],\n",
        "    target_names=LABELS,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# ---------- 8) Save ----------\n",
        "out_path = f\"qwen3_8b_{prompt_mode}_results.csv\"\n",
        "df_run.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"ğŸ’¾ Saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7dc91b5-2890-4f88-b75e-c14f7c7743de",
      "metadata": {
        "id": "a7dc91b5-2890-4f88-b75e-c14f7c7743de"
      },
      "outputs": [],
      "source": [
        "import re, math, torch, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# =====================\n",
        "# LOAD DATA\n",
        "# =====================\n",
        "DATA_PATH = \"Arabic_Reports(3000).csv\"\n",
        "TEXT_COL  = \"Ø§Ù„Ø¨Ù„Ø§Øº\"\n",
        "LABEL_COL = \"Ø§Ù„ØªØµÙ†ÙŠÙ\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH).dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "\n",
        "CLASSES = sorted(df[LABEL_COL].unique().tolist())\n",
        "\n",
        "# =====================\n",
        "# Arabic normalization\n",
        "# =====================\n",
        "def norm_ar(t: str) -> str:\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", t)\n",
        "    t = re.sub(r\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", t)\n",
        "    t = t.replace(\"Ù‰\", \"ÙŠ\").replace(\"Ø©\", \"Ù‡\")\n",
        "    t = t.replace(\"Ø¤\", \"Ùˆ\").replace(\"Ø¦\", \"ÙŠ\")\n",
        "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
        "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "df[\"_norm\"] = df[TEXT_COL].apply(norm_ar)\n",
        "\n",
        "# =====================\n",
        "# BUILD KB FROM DATA (TF-IDF)\n",
        "# =====================\n",
        "TOP_WORDS = 40\n",
        "TOP_BIGRAMS = 40\n",
        "\n",
        "class_terms = {}\n",
        "\n",
        "for cls in CLASSES:\n",
        "    texts = df[df[LABEL_COL] == cls][\"_norm\"].tolist()\n",
        "\n",
        "    vec = TfidfVectorizer(\n",
        "        ngram_range=(1,2),\n",
        "        max_features=TOP_WORDS + TOP_BIGRAMS\n",
        "    )\n",
        "    X = vec.fit_transform(texts)\n",
        "    terms = vec.get_feature_names_out()\n",
        "\n",
        "    weights = {}\n",
        "    for term in terms:\n",
        "        base = 2.0 if \" \" in term else 1.0\n",
        "        weights[term] = base\n",
        "\n",
        "    class_terms[cls] = weights\n",
        "\n",
        "# =====================\n",
        "# BOOST SIGNALS (Ø®ÙÙŠÙØ© ÙˆÙ…ØªÙˆØ§Ø²Ù†Ø©)\n",
        "# =====================\n",
        "BOOST = {\n",
        "    \"Fraud CallVishing\": [\"Ù…ÙƒØ§Ù„Ù…Ø©\",\"Ø§ØªØµÙ„\",\"otp\",\"Ø±Ù…Ø²\",\"ÙƒÙˆØ¯\",\"ØªØ­Ù‚Ù‚\"],\n",
        "    \"EWallet Related Fraud\": [\"Ø®ØµÙ…\",\"ØªØ­ÙˆÙŠÙ„\",\"Ø³Ø­Ø¨\",\"Ù…Ø¨Ù„Øº\",\"Ø±ØµÙŠØ¯\",\"Ù…Ø­ÙØ¸Ø©\",\"Ù…Ø¯Ù‰\",\"Ø§Ø¨Ù„ Ø¨Ø§ÙŠ\",\"stc pay\",\"Ø±Ø§Ø¨Ø· Ø¯ÙØ¹\"],\n",
        "    \"Profile Hacking Identity Theft\": [\"Ø§Ø®ØªØ±Ø§Ù‚\",\"ØªÙ‡ÙƒÙŠØ±\",\"ØªØºÙŠÙŠØ± ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±\",\"Ø­Ø³Ø§Ø¨ÙŠ\",\"Ø§Ù†ØªØ­Ø§Ù„\"]\n",
        "}\n",
        "\n",
        "for cls, terms in BOOST.items():\n",
        "    for t in terms:\n",
        "        t = norm_ar(t)\n",
        "        class_terms[cls][t] = class_terms[cls].get(t, 0) + 3.0\n",
        "\n",
        "# =====================\n",
        "# KB Scoring\n",
        "# =====================\n",
        "def score_kb(text):\n",
        "    t = norm_ar(text)\n",
        "    scores = {}\n",
        "    for cls, terms in class_terms.items():\n",
        "        s = 0.0\n",
        "        for term, w in terms.items():\n",
        "            if term in t:\n",
        "                s += w\n",
        "        scores[cls] = s\n",
        "\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked[0][0], ranked[1][0], ranked[0][1] - ranked[1][1]\n",
        "\n",
        "# =====================\n",
        "# Qwen Tie-breaker\n",
        "# =====================\n",
        "LABELS = CLASSES\n",
        "label_set = set(LABELS)\n",
        "\n",
        "def qwen_choose(text, a, b):\n",
        "    prompt = f\"\"\"\n",
        "Choose ONE label only:\n",
        "- {a}\n",
        "- {b}\n",
        "\n",
        "Report:\n",
        "{text}\n",
        "\n",
        "Label:\n",
        "\"\"\".strip()\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k:v.to(model.device) for k,v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=6, do_sample=False)\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred.split(\"Label:\")[-1].strip()\n",
        "\n",
        "    return a if a.lower() in pred.lower() else b\n",
        "\n",
        "# =====================\n",
        "# RUN HYBRID\n",
        "# =====================\n",
        "MARGIN_TH = 2.0\n",
        "preds = []\n",
        "\n",
        "for text in tqdm(df[TEXT_COL], total=len(df)):\n",
        "    b1, b2, margin = score_kb(text)\n",
        "    if margin < MARGIN_TH:\n",
        "        preds.append(qwen_choose(text, b1, b2))\n",
        "    else:\n",
        "        preds.append(b1)\n",
        "\n",
        "df[\"pred_label\"] = preds\n",
        "\n",
        "# =====================\n",
        "# EVAL\n",
        "# =====================\n",
        "acc = accuracy_score(df[LABEL_COL], df[\"pred_label\"])\n",
        "mf1 = f1_score(df[LABEL_COL], df[\"pred_label\"], average=\"macro\")\n",
        "\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Macro F1:  {mf1:.4f}\\n\")\n",
        "print(classification_report(df[LABEL_COL], df[\"pred_label\"], digits=4))\n",
        "\n",
        "# =====================\n",
        "# SAVE\n",
        "# =====================\n",
        "df.to_csv(\"hybrid_kb_from_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved: hybrid_kb_from_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, math, torch, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# =====================\n",
        "# LOAD DATA\n",
        "# =====================\n",
        "DATA_PATH = \"Arabic_Reports(3000).csv\"\n",
        "TEXT_COL  = \"Ø§Ù„Ø¨Ù„Ø§Øº\"\n",
        "LABEL_COL = \"Ø§Ù„ØªØµÙ†ÙŠÙ\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH).dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "\n",
        "CLASSES = sorted(df[LABEL_COL].unique().tolist())\n",
        "\n",
        "# =====================\n",
        "# Arabic normalization\n",
        "# =====================\n",
        "def norm_ar(t: str) -> str:\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", t)\n",
        "    t = re.sub(r\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", t)\n",
        "    t = t.replace(\"Ù‰\", \"ÙŠ\").replace(\"Ø©\", \"Ù‡\")\n",
        "    t = t.replace(\"Ø¤\", \"Ùˆ\").replace(\"Ø¦\", \"ÙŠ\")\n",
        "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
        "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "df[\"_norm\"] = df[TEXT_COL].apply(norm_ar)\n",
        "\n",
        "# =====================\n",
        "# BUILD KB FROM DATA (TF-IDF)\n",
        "# =====================\n",
        "TOP_WORDS = 40\n",
        "TOP_BIGRAMS = 40\n",
        "\n",
        "class_terms = {}\n",
        "\n",
        "for cls in CLASSES:\n",
        "    texts = df[df[LABEL_COL] == cls][\"_norm\"].tolist()\n",
        "\n",
        "    vec = TfidfVectorizer(\n",
        "        ngram_range=(1,2),\n",
        "        max_features=TOP_WORDS + TOP_BIGRAMS\n",
        "    )\n",
        "    X = vec.fit_transform(texts)\n",
        "    terms = vec.get_feature_names_out()\n",
        "\n",
        "    weights = {}\n",
        "    for term in terms:\n",
        "        base = 2.0 if \" \" in term else 1.0\n",
        "        weights[term] = base\n",
        "\n",
        "    class_terms[cls] = weights\n",
        "\n",
        "# =====================\n",
        "# BOOST SIGNALS (Ø®ÙÙŠÙØ© ÙˆÙ…ØªÙˆØ§Ø²Ù†Ø©)\n",
        "# =====================\n",
        "BOOST = {\n",
        "    \"Fraud CallVishing\": [\"Ù…ÙƒØ§Ù„Ù…Ø©\",\"Ø§ØªØµÙ„\",\"otp\",\"Ø±Ù…Ø²\",\"ÙƒÙˆØ¯\",\"ØªØ­Ù‚Ù‚\"],\n",
        "    \"EWallet Related Fraud\": [\"Ø®ØµÙ…\",\"ØªØ­ÙˆÙŠÙ„\",\"Ø³Ø­Ø¨\",\"Ù…Ø¨Ù„Øº\",\"Ø±ØµÙŠØ¯\",\"Ù…Ø­ÙØ¸Ø©\",\"Ù…Ø¯Ù‰\",\"Ø§Ø¨Ù„ Ø¨Ø§ÙŠ\",\"stc pay\",\"Ø±Ø§Ø¨Ø· Ø¯ÙØ¹\"],\n",
        "    \"Profile Hacking Identity Theft\": [\"Ø§Ø®ØªØ±Ø§Ù‚\",\"ØªÙ‡ÙƒÙŠØ±\",\"ØªØºÙŠÙŠØ± ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±\",\"Ø­Ø³Ø§Ø¨ÙŠ\",\"Ø§Ù†ØªØ­Ø§Ù„\"]\n",
        "}\n",
        "\n",
        "for cls, terms in BOOST.items():\n",
        "    for t in terms:\n",
        "        t = norm_ar(t)\n",
        "        class_terms[cls][t] = class_terms[cls].get(t, 0) + 3.0\n",
        "\n",
        "# =====================\n",
        "# KB Scoring\n",
        "# =====================\n",
        "def score_kb(text):\n",
        "    t = norm_ar(text)\n",
        "    scores = {}\n",
        "    for cls, terms in class_terms.items():\n",
        "        s = 0.0\n",
        "        for term, w in terms.items():\n",
        "            if term in t:\n",
        "                s += w\n",
        "        scores[cls] = s\n",
        "\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked[0][0], ranked[1][0], ranked[0][1] - ranked[1][1]\n",
        "\n",
        "# =====================\n",
        "# Qwen Tie-breaker\n",
        "# =====================\n",
        "LABELS = CLASSES\n",
        "label_set = set(LABELS)\n",
        "\n",
        "def qwen_choose(text, a, b):\n",
        "    prompt = f\"\"\"\n",
        "Choose ONE label only:\n",
        "- {a}\n",
        "- {b}\n",
        "\n",
        "Report:\n",
        "{text}\n",
        "\n",
        "Label:\n",
        "\"\"\".strip()\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k:v.to(model.device) for k,v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=6, do_sample=False)\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred.split(\"Label:\")[-1].strip()\n",
        "\n",
        "    return a if a.lower() in pred.lower() else b\n",
        "\n",
        "# =====================\n",
        "# RUN HYBRID\n",
        "# =====================\n",
        "MARGIN_TH = 2.0\n",
        "preds = []\n",
        "\n",
        "for text in tqdm(df[TEXT_COL], total=len(df)):\n",
        "    b1, b2, margin = score_kb(text)\n",
        "    if margin < MARGIN_TH:\n",
        "        preds.append(qwen_choose(text, b1, b2))\n",
        "    else:\n",
        "        preds.append(b1)\n",
        "\n",
        "df[\"pred_label\"] = preds\n",
        "\n",
        "# =====================\n",
        "# EVAL\n",
        "# =====================\n",
        "acc = accuracy_score(df[LABEL_COL], df[\"pred_label\"])\n",
        "mf1 = f1_score(df[LABEL_COL], df[\"pred_label\"], average=\"macro\")\n",
        "\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Macro F1:  {mf1:.4f}\\n\")\n",
        "print(classification_report(df[LABEL_COL], df[\"pred_label\"], digits=4))\n",
        "\n",
        "# =====================\n",
        "# SAVE\n",
        "# =====================\n",
        "df.to_csv(\"hybrid_kb_from_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved: hybrid_kb_from_data.csv\")\n"
      ],
      "metadata": {
        "id": "1vFJxmc1RQ4N"
      },
      "id": "1vFJxmc1RQ4N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, torch, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# =====================\n",
        "# LOAD DATA\n",
        "# =====================\n",
        "DATA_PATH = \"Arabic_Reports(3000).csv\"\n",
        "TEXT_COL  = \"Ø§Ù„Ø¨Ù„Ø§Øº\"\n",
        "LABEL_COL = \"Ø§Ù„ØªØµÙ†ÙŠÙ\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH).dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "CLASSES = sorted(df[LABEL_COL].unique().tolist())\n",
        "\n",
        "# =====================\n",
        "# Arabic normalization & cleaning\n",
        "# =====================\n",
        "def norm_ar(t: str) -> str:\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", t)\n",
        "    t = re.sub(r\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", t)\n",
        "    t = t.replace(\"Ù‰\", \"ÙŠ\").replace(\"Ø©\", \"Ù‡\")\n",
        "    t = t.replace(\"Ø¤\", \"Ùˆ\").replace(\"Ø¦\", \"ÙŠ\")\n",
        "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
        "    t = re.sub(r\"_x000d_\", \" \", t)  # remove extra artifacts\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "df[\"_norm\"] = df[TEXT_COL].apply(norm_ar)\n",
        "\n",
        "# =====================\n",
        "# BUILD KB FROM DATA (TF-IDF)\n",
        "# =====================\n",
        "TOP_WORDS = 40\n",
        "TOP_BIGRAMS = 40\n",
        "\n",
        "class_terms = {}\n",
        "for cls in CLASSES:\n",
        "    texts = df[df[LABEL_COL] == cls][\"_norm\"].tolist()\n",
        "    vec = TfidfVectorizer(ngram_range=(1,2), max_features=TOP_WORDS + TOP_BIGRAMS)\n",
        "    X = vec.fit_transform(texts)\n",
        "    terms = vec.get_feature_names_out()\n",
        "\n",
        "    weights = {}\n",
        "    for term in terms:\n",
        "        base = 2.0 if \" \" in term else 1.0\n",
        "        weights[term] = base\n",
        "    class_terms[cls] = weights\n",
        "\n",
        "# =====================\n",
        "# BOOST SIGNALS (Ø®ÙÙŠÙØ© ÙˆÙ…ØªÙˆØ§Ø²Ù†Ø©)\n",
        "# =====================\n",
        "BOOST = {\n",
        "    \"Fraud CallVishing\": [\"Ù…ÙƒØ§Ù„Ù…Ø©\",\"Ø§ØªØµÙ„\",\"otp\",\"Ø±Ù…Ø²\",\"ÙƒÙˆØ¯\",\"ØªØ­Ù‚Ù‚\"],\n",
        "    \"EWallet Related Fraud\": [\"Ø®ØµÙ…\",\"ØªØ­ÙˆÙŠÙ„\",\"Ø³Ø­Ø¨\",\"Ù…Ø¨Ù„Øº\",\"Ø±ØµÙŠØ¯\",\"Ù…Ø­ÙØ¸Ø©\",\"Ù…Ø¯Ù‰\",\"Ø§Ø¨Ù„ Ø¨Ø§ÙŠ\",\"stc pay\",\"Ø±Ø§Ø¨Ø· Ø¯ÙØ¹\"],\n",
        "    \"Profile Hacking Identity Theft\": [\"Ø§Ø®ØªØ±Ø§Ù‚\",\"ØªÙ‡ÙƒÙŠØ±\",\"ØªØºÙŠÙŠØ± ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±\",\"Ø­Ø³Ø§Ø¨ÙŠ\",\"Ø§Ù†ØªØ­Ø§Ù„\"]\n",
        "}\n",
        "\n",
        "BOOST_WEIGHT = 2.0  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ÙˆØ²Ù† Ù„ÙŠÙƒÙˆÙ† Ù…ØªÙˆØ§Ø²Ù†\n",
        "\n",
        "for cls, terms in BOOST.items():\n",
        "    for t in terms:\n",
        "        t = norm_ar(t)\n",
        "        class_terms[cls][t] = class_terms[cls].get(t, 0) + BOOST_WEIGHT\n",
        "\n",
        "# =====================\n",
        "# DISPLAY TOP TERMS PER CLASS\n",
        "# =====================\n",
        "print(\"Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ ÙØ¦Ø© Ø¨Ø¹Ø¯ Boost:\")\n",
        "for cls, terms in class_terms.items():\n",
        "    sorted_terms = sorted(terms.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "    print(f\"\\nØ£Ù‡Ù… 15 ÙƒÙ„Ù…Ø§Øª Ù„ÙØ¦Ø©: {cls}\")\n",
        "    for term, w in sorted_terms:\n",
        "        print(f\"{term}: {w}\")\n",
        "\n",
        "# =====================\n",
        "# KB Scoring\n",
        "# =====================\n",
        "def score_kb(text):\n",
        "    t = norm_ar(text)\n",
        "    scores = {}\n",
        "    for cls, terms in class_terms.items():\n",
        "        s = sum(w for term, w in terms.items() if term in t)\n",
        "        scores[cls] = s\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked[0][0], ranked[1][0], ranked[0][1] - ranked[1][1]\n",
        "\n",
        "# =====================\n",
        "# Qwen Tie-breaker\n",
        "# =====================\n",
        "LABELS = CLASSES\n",
        "label_set = set(LABELS)\n",
        "\n",
        "def qwen_choose(text, a, b):\n",
        "    prompt = f\"\"\"\n",
        "Choose ONE label only:\n",
        "- {a}\n",
        "- {b}\n",
        "\n",
        "Report:\n",
        "{text}\n",
        "\n",
        "Label:\n",
        "\"\"\".strip()\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k:v.to(model.device) for k,v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=6, do_sample=False)\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred.split(\"Label:\")[-1].strip()\n",
        "    return a if a.lower() in pred.lower() else b\n",
        "\n",
        "# =====================\n",
        "# RUN HYBRID\n",
        "# =====================\n",
        "MARGIN_TH = 2.0\n",
        "preds = []\n",
        "\n",
        "for text in tqdm(df[TEXT_COL], total=len(df)):\n",
        "    b1, b2, margin = score_kb(text)\n",
        "    if margin < MARGIN_TH:\n",
        "        preds.append(qwen_choose(text, b1, b2))\n",
        "    else:\n",
        "        preds.append(b1)\n",
        "\n",
        "df[\"pred_label\"] = preds\n",
        "\n",
        "# =====================\n",
        "# EVAL\n",
        "# =====================\n",
        "acc = accuracy_score(df[LABEL_COL], df[\"pred_label\"])\n",
        "mf1 = f1_score(df[LABEL_COL], df[\"pred_label\"], average=\"macro\")\n",
        "\n",
        "print(f\"\\nAccuracy:  {acc:.4f}\")\n",
        "print(f\"Macro F1:  {mf1:.4f}\\n\")\n",
        "print(classification_report(df[LABEL_COL], df[\"pred_label\"], digits=4))\n",
        "\n",
        "# =====================\n",
        "# SAVE\n",
        "# =====================\n",
        "df.to_csv(\"hybrid_kb_improved_balanced.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved: hybrid_kb_improved_balanced.csv\")\n"
      ],
      "metadata": {
        "id": "5yqHDBKmYoL-"
      },
      "id": "5yqHDBKmYoL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, torch, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# =====================\n",
        "# LOAD DATA\n",
        "# =====================\n",
        "DATA_PATH = \"Arabic_Reports(3000).csv\"\n",
        "TEXT_COL  = \"Ø§Ù„Ø¨Ù„Ø§Øº\"\n",
        "LABEL_COL = \"Ø§Ù„ØªØµÙ†ÙŠÙ\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH).dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "CLASSES = sorted(df[LABEL_COL].unique().tolist())\n",
        "\n",
        "# =====================\n",
        "# Arabic normalization\n",
        "# =====================\n",
        "def norm_ar(t: str) -> str:\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", t)\n",
        "    t = re.sub(r\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", t)\n",
        "    t = t.replace(\"Ù‰\", \"ÙŠ\").replace(\"Ø©\", \"Ù‡\")\n",
        "    t = t.replace(\"Ø¤\", \"Ùˆ\").replace(\"Ø¦\", \"ÙŠ\")\n",
        "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
        "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "df[\"_norm\"] = df[TEXT_COL].apply(norm_ar)\n",
        "\n",
        "# =====================\n",
        "# BUILD KB FROM DATA (TF-IDF)\n",
        "# =====================\n",
        "TOP_WORDS = 100\n",
        "TOP_BIGRAMS = 50\n",
        "\n",
        "class_terms = {}\n",
        "\n",
        "for cls in CLASSES:\n",
        "    texts = df[df[LABEL_COL] == cls][\"_norm\"].tolist()\n",
        "    vec = TfidfVectorizer(ngram_range=(1,3), max_features=TOP_WORDS + TOP_BIGRAMS)\n",
        "    X = vec.fit_transform(texts)\n",
        "    terms = vec.get_feature_names_out()\n",
        "\n",
        "    weights = {}\n",
        "    for term in terms:\n",
        "        base = 2.0 if \" \" in term else 1.0\n",
        "        weights[term] = base\n",
        "    class_terms[cls] = weights\n",
        "\n",
        "# =====================\n",
        "# BOOST SIGNALS (Ø®ÙÙŠÙØ© ÙˆÙ…ØªÙˆØ§Ø²Ù†Ø©)\n",
        "# =====================\n",
        "BOOST = {\n",
        "    \"Fraud CallVishing\": [\"Ù…ÙƒØ§Ù„Ù…Ø©\",\"Ø§ØªØµÙ„\",\"otp\",\"Ø±Ù…Ø²\",\"ÙƒÙˆØ¯\",\"ØªØ­Ù‚Ù‚\",\"whatsapp\"],\n",
        "    \"EWallet Related Fraud\": [\"Ø®ØµÙ…\",\"ØªØ­ÙˆÙŠÙ„\",\"Ø³Ø­Ø¨\",\"Ù…Ø¨Ù„Øº\",\"Ø±ØµÙŠØ¯\",\"Ù…Ø­ÙØ¸Ù‡\",\"Ù…Ø¯ÙŠ\",\"Ø§Ø¨Ù„ Ø¨Ø§ÙŠ\",\"stc pay\",\"Ø±Ø§Ø¨Ø· Ø¯ÙØ¹\"],\n",
        "    \"Profile Hacking Identity Theft\": [\"Ø§Ø®ØªØ±Ø§Ù‚\",\"ØªÙ‡ÙƒÙŠØ±\",\"ØªØºÙŠÙŠØ± ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±\",\"Ø­Ø³Ø§Ø¨ÙŠ\",\"Ø§Ù†ØªØ­Ø§Ù„\"]\n",
        "}\n",
        "\n",
        "# Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ÙˆØ²Ù† Ù„Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø¹ÙŠÙØ© Fraud CallVishing\n",
        "BOOST_WEIGHTS = {\"Fraud CallVishing\": 4.0, \"EWallet Related Fraud\": 3.0, \"Profile Hacking Identity Theft\": 3.0}\n",
        "\n",
        "for cls, terms in BOOST.items():\n",
        "    w = BOOST_WEIGHTS.get(cls, 3.0)\n",
        "    for t in terms:\n",
        "        t = norm_ar(t)\n",
        "        class_terms[cls][t] = class_terms[cls].get(t, 0) + w\n",
        "\n",
        "# =====================\n",
        "# KB Scoring\n",
        "# =====================\n",
        "def score_kb(text):\n",
        "    t = norm_ar(text)\n",
        "    scores = {}\n",
        "    for cls, terms in class_terms.items():\n",
        "        s = 0.0\n",
        "        for term, w in terms.items():\n",
        "            if term in t:\n",
        "                s += w\n",
        "        scores[cls] = s\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked[0][0], ranked[1][0], ranked[0][1] - ranked[1][1]\n",
        "\n",
        "# =====================\n",
        "# Qwen Tie-breaker\n",
        "# =====================\n",
        "LABELS = CLASSES\n",
        "label_set = set(LABELS)\n",
        "\n",
        "def qwen_choose(text, a, b):\n",
        "    prompt = f\"\"\"\n",
        "Choose ONE label only:\n",
        "- {a}\n",
        "- {b}\n",
        "\n",
        "Report:\n",
        "{text}\n",
        "\n",
        "Label:\n",
        "\"\"\".strip()\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k:v.to(model.device) for k,v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=6, do_sample=False)\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = pred.split(\"Label:\")[-1].strip()\n",
        "    return a if a.lower() in pred.lower() else b\n",
        "\n",
        "# =====================\n",
        "# RUN HYBRID\n",
        "# =====================\n",
        "MARGIN_TH = 2.0\n",
        "preds = []\n",
        "\n",
        "for text in tqdm(df[TEXT_COL], total=len(df)):\n",
        "    b1, b2, margin = score_kb(text)\n",
        "    if margin < MARGIN_TH:\n",
        "        preds.append(qwen_choose(text, b1, b2))\n",
        "    else:\n",
        "        preds.append(b1)\n",
        "\n",
        "df[\"pred_label\"] = preds\n",
        "\n",
        "# =====================\n",
        "# EVAL\n",
        "# =====================\n",
        "acc = accuracy_score(df[LABEL_COL], df[\"pred_label\"])\n",
        "mf1 = f1_score(df[LABEL_COL], df[\"pred_label\"], average=\"macro\")\n",
        "\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Macro F1:  {mf1:.4f}\\n\")\n",
        "print(classification_report(df[LABEL_COL], df[\"pred_label\"], digits=4))\n",
        "\n",
        "# =====================\n",
        "# SAVE\n",
        "# =====================\n",
        "df.to_csv(\"hybrid_kb_enhanced.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved: hybrid_kb_enhanced.csv\")\n"
      ],
      "metadata": {
        "id": "x1sGUE76brBr"
      },
      "id": "x1sGUE76brBr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafbdc88-dd8b-488e-b543-7c338771248b",
      "metadata": {
        "id": "cafbdc88-dd8b-488e-b543-7c338771248b"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# Ø¹Ø±Ø¶ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ ÙØ¦Ø©\n",
        "# =====================\n",
        "TOP_N = 15  # Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¹Ø±Ø¶Ù‡Ø§ Ù„ÙƒÙ„ ÙØ¦Ø©\n",
        "\n",
        "for cls, terms in class_terms.items():\n",
        "    print(f\"\\nØ£Ù‡Ù… {TOP_N} ÙƒÙ„Ù…Ø§Øª Ù„ÙØ¦Ø©: {cls}\")\n",
        "    # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ÙˆØ²Ù† Ù…Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰ Ù„Ù„Ø£Ù‚Ù„\n",
        "    sorted_terms = sorted(terms.items(), key=lambda x: x[1], reverse=True)\n",
        "    for term, weight in sorted_terms[:TOP_N]:\n",
        "        print(f\"{term}: {weight}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# =====================\n",
        "# LOAD DATA\n",
        "# =====================\n",
        "DATA_PATH = \"Arabic_Reports(3000).csv\"\n",
        "TEXT_COL  = \"Ø§Ù„Ø¨Ù„Ø§Øº\"\n",
        "LABEL_COL = \"Ø§Ù„ØªØµÙ†ÙŠÙ\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH).dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "CLASSES = sorted(df[LABEL_COL].unique().tolist())\n",
        "\n",
        "# =====================\n",
        "# Arabic normalization\n",
        "# =====================\n",
        "def norm_ar(t: str) -> str:\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", t)\n",
        "    t = re.sub(r\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", t)\n",
        "    t = t.replace(\"Ù‰\", \"ÙŠ\").replace(\"Ø©\", \"Ù‡\")\n",
        "    t = t.replace(\"Ø¤\", \"Ùˆ\").replace(\"Ø¦\", \"ÙŠ\")\n",
        "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
        "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "df[\"_norm\"] = df[TEXT_COL].apply(norm_ar)\n",
        "\n",
        "# =====================\n",
        "# Arabic stopwords & cleaning\n",
        "# =====================\n",
        "STOPWORDS = [\n",
        "    \"Ù…Ù†\", \"Ø¹Ù„Ù‰\", \"Ø§Ù„Ù‰\", \"ÙÙŠ\", \"Ø¹Ù†\", \"Ù…Ø§\", \"Ù„Ù…\", \"Ù„Ø§\", \"Ø§Ù„Ø°ÙŠ\", \"Ù‡Ø°Ù‡\", \"Ø°Ù„Ùƒ\",\n",
        "    \"Ø§Ø±Ø¬Ùˆ\", \"Ù…Ù†ÙƒÙ…\", \"ØªÙ…\", \"Ù…Ù†\", \"Ø®Ù„Ø§Ù„\", \"Ù…Ø¹\", \"Ø¹Ù„ÙŠ\", \"Ø§Ù„Ù‰\", \"_x000d_\"\n",
        "]\n",
        "\n",
        "def clean_terms(term_list):\n",
        "    return [t for t in term_list if t not in STOPWORDS]\n",
        "\n",
        "# =====================\n",
        "# BUILD KB FROM DATA (TF-IDF)\n",
        "# =====================\n",
        "TOP_WORDS = 50  # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠØ©\n",
        "TOP_BIGRAMS = 50  # Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ©\n",
        "\n",
        "class_terms = {}\n",
        "\n",
        "for cls in CLASSES:\n",
        "    texts = df[df[LABEL_COL] == cls][\"_norm\"].tolist()\n",
        "\n",
        "    vec = TfidfVectorizer(\n",
        "        ngram_range=(1,2),\n",
        "        max_features=TOP_WORDS + TOP_BIGRAMS\n",
        "    )\n",
        "    X = vec.fit_transform(texts)\n",
        "    terms = clean_terms(vec.get_feature_names_out())\n",
        "\n",
        "    # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: 2 Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ©ØŒ 1 Ù„Ù„Ø£Ø­Ø§Ø¯ÙŠØ©\n",
        "    weights = {term: (2.0 if \" \" in term else 1.0) for term in terms}\n",
        "\n",
        "    class_terms[cls] = weights\n",
        "\n",
        "# =====================\n",
        "# BOOST ÙƒÙ„Ù…Ø§Øª Ù…Ù‡Ù…Ø©\n",
        "# =====================\n",
        "BOOST = {\n",
        "    \"Fraud CallVishing\": [\"Ù…ÙƒØ§Ù„Ù…Ø©\",\"Ø§ØªØµÙ„\",\"otp\",\"Ø±Ù…Ø²\",\"ÙƒÙˆØ¯\",\"ØªØ­Ù‚Ù‚\"],\n",
        "    \"EWallet Related Fraud\": [\"Ø®ØµÙ…\",\"ØªØ­ÙˆÙŠÙ„\",\"Ø³Ø­Ø¨\",\"Ù…Ø¨Ù„Øº\",\"Ø±ØµÙŠØ¯\",\"Ù…Ø­ÙØ¸Ø©\",\"Ù…Ø¯Ù‰\",\"Ø§Ø¨Ù„ Ø¨Ø§ÙŠ\",\"stc pay\",\"Ø±Ø§Ø¨Ø· Ø¯ÙØ¹\"],\n",
        "    \"Profile Hacking Identity Theft\": [\"Ø§Ø®ØªØ±Ø§Ù‚\",\"ØªÙ‡ÙƒÙŠØ±\",\"ØªØºÙŠÙŠØ± ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±\",\"Ø­Ø³Ø§Ø¨ÙŠ\",\"Ø§Ù†ØªØ­Ø§Ù„\"]\n",
        "}\n",
        "\n",
        "for cls, terms in BOOST.items():\n",
        "    for t in terms:\n",
        "        t = norm_ar(t)\n",
        "        class_terms[cls][t] = class_terms[cls].get(t, 0) + 3.0\n",
        "\n",
        "# =====================\n",
        "# Ø¹Ø±Ø¶ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„Ù€ Boost\n",
        "# =====================\n",
        "TOP_N = 15\n",
        "for cls, terms in class_terms.items():\n",
        "    print(f\"\\nØ£Ù‡Ù… {TOP_N} ÙƒÙ„Ù…Ø§Øª Ù„ÙØ¦Ø©: {cls}\")\n",
        "    sorted_terms = sorted(terms.items(), key=lambda x: x[1], reverse=True)\n",
        "    for term, weight in sorted_terms[:TOP_N]:\n",
        "        print(f\"{term}: {weight}\")\n"
      ],
      "metadata": {
        "id": "tYJwCUyyTltW"
      },
      "id": "tYJwCUyyTltW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (ibex_jupyter)",
      "language": "python",
      "name": "ibex_jupyter"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}